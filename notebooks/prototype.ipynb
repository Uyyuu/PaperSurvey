{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/PaperSurvey\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "WORK_DIR = os.environ.get(\"WORK_DIR\")\n",
    "print(WORK_DIR)\n",
    "\n",
    "sys.path.append(WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /PaperSurvey/docs/prototype/Learning Community Embedding with Community Detection and Node Embedding on Graphs.pdf...\n",
      "[                                        ] (0/1===[====                                    ] ( 1/10===[========                                ] ( 2/10===[============                            ] ( 3/10===[================                        ] ( 4/10===[====================                    ] ( 5/10===[========================                ] ( 6/10===[============================            ] ( 7/10===[================================        ] ( 8/10===[====================================    ] ( 9/10===[========================================] (10/10]\n",
      "# Learning Community Embedding with Community Detection and Node Embedding on Graphs\n",
      "\n",
      "\n",
      "## Sandro Cavallari\n",
      "#### Nanyang Technological University Singapore sandro001@e.ntu.edu.sg\n",
      "\n",
      "\n",
      "## Vincent W. Zheng\n",
      "#### Advanced Digital Sciences Center Singapore vincent.zheng@adsc.com.sg\n",
      "\n",
      "\n",
      "## Hongyun Cai\n",
      "#### Advanced Digital Sciences Center Singapore hongyun.c@adsc.com.sg\n",
      "\n",
      "\n",
      "## Kevin Chen-Chuan Chang\n",
      "#### University of Illinois at Urbana-Champaign IL, USA kcchang@illinois.edu\n",
      "\n",
      "### ABSTRACT\n",
      "\n",
      "In this paper, we study an important yet largely under-explored\n",
      "setting of graph embedding, i.e., embedding communities instead\n",
      "of each individual nodes. We find that community embedding is\n",
      "not only useful for community-level applications such as graph\n",
      "visualization, but also beneficial to both community detection and\n",
      "node classification. To learn such embedding, our insight hinges\n",
      "upon a closed loop among community embedding, community detection and node embedding. On the one hand, node embedding\n",
      "can help improve community detection, which outputs good communities for fitting better community embedding. On the other\n",
      "hand, community embedding can be used to optimize the node embedding by introducing a community-aware high-order proximity.\n",
      "Guided by this insight, we propose a novel community embedding\n",
      "framework that jointly solves the three tasks together. We evaluate\n",
      "such a framework on multiple real-world datasets, and show that\n",
      "it improves graph visualization and outperforms state-of-the-art\n",
      "baselines in various application tasks, e.g., community detection\n",
      "and node classification.\n",
      "\n",
      "### CCS CONCEPTS\n",
      "\n",
      "- Computing methodologies → **Neural networks; Machine**\n",
      "_learning algorithms; • Mathematics of computing →_ _Probabilis-_\n",
      "_tic algorithms; • Applied computing →_ _Sociology;_\n",
      "\n",
      "### KEYWORDS\n",
      "\n",
      "community embedding, graph embedding\n",
      "\n",
      "### 1 INTRODUCTION\n",
      "\n",
      "Traditionally, graph embedding focuses on individual nodes and\n",
      "aims to output a vector representation for each node in the graph,\n",
      "\n",
      "Permission to make digital or hard copies of all or part of this work for personal or\n",
      "classroom use is granted without fee provided that copies are not made or distributed\n",
      "for profit or commercial advantage and that copies bear this notice and the full citation\n",
      "on the first page. Copyrights for components of this work owned by others than ACM\n",
      "must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\n",
      "to post on servers or to redistribute to lists, requires prior specific permission and/or a\n",
      "fee. Request permissions from permissions@acm.org.\n",
      "_CIKM’17, November 6–10, 2017, Singapore, Singapore_\n",
      "© 2017 Association for Computing Machinery.\n",
      "ACM ISBN 978-1-4503-4918-5/17/11...$15.00\n",
      "[https://doi.org/10.1145/3132847.3132925](https://doi.org/10.1145/3132847.3132925)\n",
      "\n",
      "\n",
      "## Erik Cambria\n",
      "#### Nanyang Technological University Singapore cambria@ntu.edu.sg\n",
      "\n",
      "(a) Karate club graph (b) Visualization based on our model\n",
      "\n",
      "**Figure 1: Embedding nodes and communities in a 2D space.**\n",
      "\n",
      "such that two nodes “close” on the graph have similar vector representations in a low-dimensional space. Such node embedding has\n",
      "been shown very successful in preserving the network structure,\n",
      "and significantly improving a wide range of applications, including\n",
      "node classification [5, 20], node clustering [27, 34], link prediction [12, 19], graph visualization [24, 29] and more [10, 18].\n",
      "In this paper, we study another important, yet largely underexplored setting of graph embedding, which focuses on embedding\n",
      "_communities. Generally, a “community embedding” is a represen-_\n",
      "tation for a community in a low-dimensional space. Because a\n",
      "community is a group of densely connected nodes, a community\n",
      "embedding is expected to characterize how its member nodes distribute in the low-dimensional space. As a result, we cannot simply\n",
      "define a community embedding as a vector; instead, we need to\n",
      "define it as a distribution in the low-dimensional space. In Fig. 1, we\n",
      "use the well-studied Karate Club graph[1] as an example to demonstrate community embedding in a 2D space. As shown in Fig. 1(a),\n",
      "the Karate Club graph has 34 nodes and 78 edges. It is known that\n",
      "this graph has two communities, one of which is led by a class\n",
      "instructor (node 1) and the other of which is led by a club administrator (node 34) [35]. Some club members (e.g., node 9) are identified\n",
      "as “weak supporters” to the two communities, thus they can belong\n",
      "to both. In Fig. 1(b), we visualize the graph in the 2D space, where\n",
      "each node embedding is a 2D vector.\n",
      "\n",
      "[1https://networkdata.ics.uci.edu/data.php?id=105](https://networkdata.ics.uci.edu/data.php?id=105)\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**Figure 2: A closed loop for learning community embedding.**\n",
      "\n",
      "Because each community is a group of densely connected nodes,\n",
      "we are motivated by the Gaussian mixture model (GMM) [3] to\n",
      "see each community embedding as a multivariate Gaussian distri_bution in the 2D space. Consequently, we visualize the two over-_\n",
      "lapping communities in the Karate Club graph as two overlapping\n",
      "eclipses, each of which is characterized by a 2D mean vector and\n",
      "a 2 × 2 covariance matrix. Community embedding is useful for\n",
      "many community-level applications, e.g., for community visualization to help generate insights from big graphs, or community\n",
      "recommendation to search for similar communities.\n",
      "Learning community embedding is non-trivial. On the one hand,\n",
      "to have meaningful community embedding, we first need to well\n",
      "identify the communities. Then, a straightforward approach for\n",
      "community embedding is to: (1) run community detection, such\n",
      "as Spectral Clustering [25], on the graph to get community assignments for each node; (2) apply node embedding, such as DeepWalk [20] or LINE [24], on the graph to get an embedding vector\n",
      "for each node; (3) aggregate the node embedding vectors in each\n",
      "community, so as to fit a (multivariate Gaussian) distribution as its\n",
      "community embedding. Such a pipeline approach is suboptimal,\n",
      "because its community detection is independent of its node embedding. On the other hand, recent studies show that node embedding\n",
      "often improves community detection, thanks to its well preserving\n",
      "the network structure in a low-dimensional space [5, 15, 27]. Hence,\n",
      "another possible approach for community embedding is to directly\n",
      "run community detection over the node embedding results and,\n",
      "hence, fit a (multivariate Gaussian) distribution for each community\n",
      "based on its node embedding vectors. However, such an approach\n",
      "is also suboptimal, because most of the existing node embedding\n",
      "methods (e.g., DeepWalk [20], LINE[24] and Node2Vec [12]) are\n",
      "not aware of community structure, which makes their node embedding inputs suboptimal for the subsequent community detection. In\n",
      "Sec. 5, we empirically evaluate both of the above approaches, and\n",
      "show that their performances are limited. There is few work that\n",
      "considers node embedding and community detection together; they\n",
      "either require extra supervision (e.g., must-links) [34] or high computational complexity (e.g., quadratic to the number of nodes in a\n",
      "graph) [31]. Moreover, they under-characterize a community in the\n",
      "low-dimensional space as a vector, thus it is difficult to accurately\n",
      "visualize overlapping communities.\n",
      "Our insight for learning community embedding is that, there\n",
      "exists a closed loop among community detection, community embedding and node embedding, as shown in Fig. 2. On the one hand,\n",
      "as discussed earlier, node embedding can help improve community detection (i.e., ①), which outputs good communities for fitting\n",
      "meaningful community embedding (i.e., ②). On the other hand,\n",
      "community embedding can be used to optimize node embedding\n",
      "(i.e., ③). Suppose for a community k, we already have its community embedding as a multivariate distribution in a low-dimensional\n",
      "space. Then, we can enforce community k’s member nodes to scatter closely near its community embedding’s mean vector in that\n",
      "\n",
      "\n",
      "low-dimensional space. As a result, these same-community nodes\n",
      "tend to have similar node embedding vectors. Compared with firstand second-order proximity, community embedding does not require two nodes to be directly linked or share many “contexts”\n",
      "for being close. Because the connections between two nodes in a\n",
      "community can be high-order, we consider community embedding\n",
      "as introducing a community-aware high-order proximity to node\n",
      "embedding. This feedback from community embedding to node embedding helps us to close the loop; hopefully the community-aware\n",
      "node embedding can serve as better inputs for the subsequent community detection, thus leading to more meaningful community\n",
      "embedding results.\n",
      "Guided by the closed loop insight, we propose ComE, a novel\n",
      "Community Embedding framework that jointly solves community\n",
      "embedding, community detection and node embedding together.\n",
      "We define community embedding as a multivariate Gaussian distribution, and use it to empower community detection from the node\n",
      "embedding results by a Gaussian mixture formulation. Denote a\n",
      "graph as G = (V, E), where V is the set of nodes and E is the set of\n",
      "edges. This Gaussian mixture formulation enables us to efficiently\n",
      "detect the communities and infer their community embedding distributions from G in O(|V |) time. Given community assignments\n",
      "and community embedding, we extend the neural network formulations of DeepWalk and LINE to preserve first-, second- and\n",
      "high-order (community-aware) proximity together. For this neural\n",
      "network formulation, we propose a scalable inference algorithm,\n",
      "whose complexity is linear to the graph size O(|V | + |E|).\n",
      "We summarize our contributions as follows.\n",
      "\n",
      "- We introduce a novel joint modeling framework, which leverages\n",
      "the closed loop among node embedding, community detection and\n",
      "community embedding, to learn graph embedding.\n",
      "\n",
      "- We contribute with a scalable inference algorithm which complexity of O(|V | + |E|), is often lower than the existing higher-order\n",
      "proximity-aware methods (Tab. 1).\n",
      "\n",
      "- We evaluate ComE on multiple real-world datasets with various\n",
      "application tasks. It renders better graph visualization results, and\n",
      "also improves the state-of-the-art baselines by at least 6.6% (NMI)\n",
      "and 2.2%–16.9% (conductance) in community detection, 0.8%–26.9%\n",
      "(macro-F1) and 0.71%–48% (micro-F1) in node classification.\n",
      "\n",
      "### 2 RELATED WORK\n",
      "\n",
      "We summarize the differences of our work with some representative\n",
      "related work on graph embedding and community detection in\n",
      "Tab. 1. Next we will detail the discussion of related work.\n",
      "\n",
      "### 2.1 Graph Embedding\n",
      "\n",
      "As there is an increasing amount of graph data, ranging from social\n",
      "networks to various information networks, an important question\n",
      "arises is how to represent a graph for analytics [11]. Graph embedding is the state-of-the-art graph representation framework, which\n",
      "aims to project a graph into a low-dimensional space for further\n",
      "applications [16, 20, 26]. In terms of the target to embed, most of\n",
      "the existing graph embedding methods focus on nodes. For example, earlier methods, such as MDS [7], LLE [21], IsoMap [26] and\n",
      "Laplacian eigenmap [2], aim to preserve the first-order proximity\n",
      "extracting the leading eigenvectors of a graph affinity matrices.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**Table 1: Comparison with related work. In the following**\n",
      "**analysis are reported only factors dependent to the graph.**\n",
      "\n",
      "node community community model\n",
      "embed. embed. detection complexity\n",
      "DeepWalk [20] - _O_ (|V | log |V |)\n",
      "LINE [24]  - _O_ (a |E |)\n",
      "Node2Vec [12] - _O_ (|V | log |V | + |V |a[2])\n",
      "GraRep [5]  - _O_ (|V |[3])\n",
      "Spectral [25]  - _O_ (|V |[3])\n",
      "DNR [34]  -  - _O_ (|V |[2])\n",
      "M-NMF [31]  -  - _O_ (|V |[2])\n",
      "ComE   -   -   - _O_ (|V | + |E |)\n",
      "\n",
      "More recent methods start to exploit neural networks to learn\n",
      "the representation for each node, with either shallow architectures [12, 24, 33] or deep architectures [1, 8, 18, 29]. DeepWalk [20]\n",
      "models the second-order proximity for node embedding with path\n",
      "sampling, and its complexity using hierarchical softmax[2] for inference is O(|V | log |V |). Node2Vec [12] extends DeepWalk with a\n",
      "controlled path sampling process, which requires O(|V |a[2]) where\n",
      "_a is the average degree of the graph; thus, its model complexity is_\n",
      "_O(|V | log |V | + |V |a[2]). LINE [24] and SDNE [29] preserve both first-_\n",
      "and second-order proximity at the price of a higher complexity,\n",
      "respectively O(a|E|) and O(a|V |).\n",
      "Compared with our methods, the above works have a lower\n",
      "or comparable complexity, but neither try to detect nor represent\n",
      "the communities. Community structure is known as an important\n",
      "network property, and it has been considered in node embedding.\n",
      "For example, in SAE [27], the authors show that spectral clustering\n",
      "can be regarded as reconstructing a graph’s normalized similarity\n",
      "matrix, but it is expensive with a complexity of at least O(|V |[2][.][367]).\n",
      "Thus, they propose to directly construct the normalized similarity\n",
      "matrix with _O(|E|) complexity, and input it to stacked Auto-Encoder_\n",
      "for reconstruction with O(|V |) complexity. The resulting node embedding is used for K-means clustering and is shown to obtain\n",
      "better communities than spectral clustering. Similarly, DNR [34]\n",
      "constructs a modularity matrix from the graph with O(|V |[2]) complexity, then applies stacked Auto-Encoder to the modularity matrix\n",
      "for node embedding. It also introduces must-links to supervise the\n",
      "node embedding.\n",
      "Higher-order of proximity methods, such as GraRep [5] and\n",
      "HOPE [19], are not explicitly community aware. Besides this, GraRep\n",
      "learn a high-order transition probability matrices and later run Singular Value Decomposition (SVD) for a O(|V |[3]) complexity. The\n",
      "above-mentioned neither tries to embed communities, nor explicitly detects communities in node embedding, but generally have a\n",
      "higher complexity due to the sparsity of real work network. There\n",
      "is little work that tries to explicitly embed communities in a lowdimensional space. For example, M-NMF [31] constructs the modularity matrix with O(|V |[2]) complexity, then applies non-negative\n",
      "matrix factorization to learn node embedding and community detection together with a complexity proportional to O(|V |[2]).\n",
      "Comparatively, M-NMF represents each community with a vector, thous we would not consider it to produce a community embedding; besides its complexity is generally higher than our complexity of O(|V | + |E|), since in practice the graphs are sparse with\n",
      "|E| ≪|V |[2].\n",
      "\n",
      "2If using negative sampling, the complexity becomes O (|V |).\n",
      "\n",
      "\n",
      "**Table 2: Notations used in this paper.**\n",
      "\n",
      "**Notation** **Description**\n",
      "_G(V_, E) Graph G, nodes V and edges E\n",
      "ℓ Length of each random walk path in sampling\n",
      "_γ_ Number of random walks for each node in sampling\n",
      "_ζ_ Context size\n",
      "_m_ Negative context size\n",
      "_ϕ_ _i ∈_ R[d] Node embedding of node i\n",
      "_ϕ[′]i_ [∈] [R][d] Context embedding of node i\n",
      "N(ψk, Σk ) Community embedding of community k\n",
      "_ψk ∈_ R[d] Community embedding k’s mean vector\n",
      "Σk ∈ R[d] [×][d] Community embedding k’s covariance matrix\n",
      "_πik ∈[0, 1]_ Community membership of node i to community k\n",
      "_Pn_ (·) Negative sampling probability\n",
      "_K_ Number of communities on G\n",
      "_α_ Trade-off parameter for context embedding\n",
      "_β_ Trade-off parameter for community embedding\n",
      "_a_ graph’s average degree\n",
      "\n",
      "### 2.2 Community Detection\n",
      "\n",
      "Community detection aims to discover groups of nodes on a graph,\n",
      "such that the intra-group connections are denser than the intergroup ones [30]. With the prevalence of social networks, recent\n",
      "community detection studies start to exploit rich node interactions\n",
      "on the graphs, such as nodes with content [22], attributes [23] and\n",
      "node-to-node diffusion [4]. A comprehensive survey of recent community detection algorithms can be found in [32]. In this work, our\n",
      "community detection is applied to homogeneous graphs, whose\n",
      "nodes and edges do not have additional information. Earlier community detection methods on homogeneous graphs often apply\n",
      "different clustering algorithms directly on the graph adjacency\n",
      "matrix. For example, in [25], spectral clustering is applied to the\n",
      "social networks for extracting the communities. In [13], a Laplacian\n",
      "Regularized GMM is trained to capture the manifold structure of a\n",
      "nearest neighbor graph.\n",
      "With the recent development of neural networks and deep learning, node embedding is utilized to assist community detection [15,\n",
      "27]. Such work usually first embeds the graph in a low-dimensional\n",
      "space, and then apply clustering algorithms such as K-means on the\n",
      "embedding results. Despite the success of these node embedding\n",
      "based methods in detecting communities, they often do not jointly\n",
      "optimize node embedding and community detection together. As\n",
      "their goals are mainly community detection, they do not necessarily\n",
      "have an explicit notion of community embedding.\n",
      "\n",
      "### 3 PROBLEM FORMULATION\n",
      "\n",
      "As input, we are given a graph G = (V, E), where V is the node\n",
      "set and E is the edge set. Traditional graph embedding aims to\n",
      "learn a node embedding for each vi ∈ _V as ϕi ∈_ R[d] . In this paper,\n",
      "we also try to learn community embedding. Suppose there are\n",
      "_K communities on the graph G. For each node vi_, we denote its\n",
      "community assignment as zi ∈{1, ..., _K_ }.\n",
      "Motivated by the Gaussian mixture formulation [3], we define\n",
      "community embedding as a multivariate Gaussian distribution in a\n",
      "low-dimensional space.\n",
      "\n",
      "|Col1|node embed.|community embed.|community detection|model complexity|\n",
      "|---|---|---|---|---|\n",
      "|DeepWalk [20] LINE [24] Node2Vec [12] GraRep [5] Spectral [25] DNR [34] M-NMF [31] ComE|• • • • • • •|•|• • • •|O(|V | log |V |) O(a |E |) O(|V | log |V | + |V |a2) O(|V |3) O(|V |3) O(|V |2) O(|V |2) O(|V | + |E |)|\n",
      "\n",
      "|Notation|Description|\n",
      "|---|---|\n",
      "|G(V, E)|Graph G, nodes V and edges E|\n",
      "|ℓ|Length of each random walk path in sampling|\n",
      "|γ|Number of random walks for each node in sampling|\n",
      "|ζ|Context size|\n",
      "|m|Negative context size|\n",
      "|ϕ ∈Rd i|Node embedding of node i|\n",
      "|ϕ′ ∈Rd i|Context embedding of node i|\n",
      "|N(ψ , Σ ) k k|Community embedding of community k|\n",
      "|ψ ∈Rd k|Community embedding k’s mean vector|\n",
      "|Σ ∈Rd×d k|Community embedding k’s covariance matrix|\n",
      "|π ∈[0, 1] ik|Community membership of node i to community k|\n",
      "|Pn(·)|Negative sampling probability|\n",
      "|K|Number of communities on G|\n",
      "|α|Trade-off parameter for context embedding|\n",
      "|β|Trade-off parameter for community embedding|\n",
      "|a|graph’s average degree|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "_Definition 3.1. Community embedding of a community_ _k (with_\n",
      "_k ∈{1, ...,_ _K_ }) in a d-dimensional space is a multivariate Gaussian distribution N(ψk, Σk ), where ψk ∈ R[d] is a mean vector and\n",
      "Σk ∈ R[d][×][d] is a covariance matrix.\n",
      "\n",
      "As output, we aim to learn: (1) node embedding ϕi for each node\n",
      "_vi ∈_ _V ; (2) community membership πik_, such that [�]k[K]=1 _[π][ik][ =][ 1,]_\n",
      "for each node vi ∈ _V and each community k ∈{1, ...,_ _K_ }; (3) community embedding parameters (ψk, Σk ) for each community k\n",
      "∈{1, ..., _K_ }.\n",
      "We summarize all of our notations in Tab. 2.\n",
      "\n",
      "### 3.1 Community Detection and Embedding\n",
      "\n",
      "Given node embedding, one straightforward way to detect communities and learn their community embedding is to take a pipeline\n",
      "approach. For example, as shown in Fig. 2, we can run Spectral\n",
      "Clustering to detect communities, then fit a Gaussian mixture for\n",
      "each community. However, such a pipeline approach lacks a unified\n",
      "objective function, thus, being hard to optimize later with node\n",
      "embedding. Alternatively, we can do community detection and embedding together in one single objective function based on GMM.\n",
      "That is, we consider each node _vi_ ’s embedding ϕi as generated by a\n",
      "multivariate Gaussian distribution from a community zi = k. Then,\n",
      "for all the nodes in V, we have the likelihood as\n",
      "�i|V= |1 �kK=1 _[p][(][z][i][ =][ k][)][p][(][v][i][ |][z][i][ =][ k][;]_ _[ϕ][i]_ [,][ψ][k] [,][ Σ][k] [)] (1)\n",
      "\n",
      "where p(zi = k) is the probability of node vi belonging to community k. For notation simplicity, we denote p(zi = k) as πik ; thus,\n",
      "we have πik ∈[0, 1] and [�]k[K]=1 _[π][ik][ =][ 1. In community detection,]_\n",
      "these πik ’s indicate the mixed community membership for each\n",
      "node vi, and they are unknown. Besides, p(vi |zi = k; _ϕi_,ψk, Σk ) is\n",
      "a multivariate Gaussian distribution defined as follows\n",
      "\n",
      "_p(vi |zi = k;_ _ϕi_,ψk, Σk ) = N(ϕi |ψk, Σk ) (2)\n",
      "\n",
      "In community embedding, the (ψk, Σk )’s are unknown. By optimizing Eq. 1 w.r.t. πik ’s and (ψk, Σk )’s, we achieve community\n",
      "detection and embedding at the same time.\n",
      "\n",
      "### 3.2 Node Embedding\n",
      "\n",
      "Traditionally, node embedding focuses on preserving first- or secondorder proximity. For example, to preserve first-order proximity,\n",
      "LINE [24] enforces two neighboring nodes to have similar embedding by minimizing\n",
      "\n",
      "_O1 = −_ [�](vi,vj )∈E [log] _[σ]_ [(][ϕ][T]j _[ϕ][i]_ [)] (3)\n",
      "\n",
      "where σ (x) = 1/(1 + exp(−x)) is a sigmoid function. To preserve\n",
      "second-order proximity, LINE and DeepWalk [20] both enforce two\n",
      "nodes sharing many “contexts” (i.e., neighbors within ζ hops) to\n",
      "have similar embedding. In this case, each node has two roles: a\n",
      "node for itself and a context for some other nodes. To differentiate\n",
      "such roles, DeepWalk introduces an extra context embedding for\n",
      "each node vj as ϕ _j[′]_ [∈] [R][d] [. Denote][ C][i][ as the set of contexts for][ v][i] [.]\n",
      "Then, we adopt negative sampling [17] to define a function for\n",
      "measuring how well vi generates each of its contexts vj ∈ _Ci as_\n",
      "\n",
      "∆ij = log _σ_ (ϕ [′][T]j _[ϕ]i_ [)][ +][ �][m]t =1 [E][v]l [∼][P]n [(][v]l [)][[][log] _[σ]_ [(−][ϕ] [′][T]l _[ϕ][i]_ [)]] (4)\n",
      "\n",
      "\n",
      "where vl ∼ _Pn_ (vl ) denotes sampling a node vl ∈ _V as a “negative_\n",
      "context” of vi according to a probability Pn (vl ). We set Pn (vl ) ∝\n",
      "_rl[3][/][4]_ as proposed in [17], where rl is vl ’s degree. In total, there are\n",
      "_m negative contexts. Generally, maximizing Eq. 4 enforces node_\n",
      "_vi_ ’s embedding ϕi to best generate its positive contexts ϕ _j[′][’s, but]_\n",
      "not its negative contexts ϕ [′]\n",
      "_l_ [’s. Then, we can minimize the following]\n",
      "objective function to preserve the second-order proximity:\n",
      "\n",
      "_O2 = −α_ [�]vi ∈V �vj ∈Ci [∆]ij (5)\n",
      "\n",
      "where α > 0 is a trade-off parameter.\n",
      "\n",
      "### 3.3 Closing the Loop\n",
      "\n",
      "In order to close the loop in Fig. 2, we need to enable the feedback from community detection and community embedding to\n",
      "node embedding. Suppose we have identified the mixed community membership πik ’s and the community embedding (ψk, Σk )’s\n",
      "in Sec. 3.1. Then, we can re-use Eq. 1 to enable such feedback, by\n",
      "seeing the node embedding ϕi ’s as unknown. Effectively, optimizing\n",
      "Eq. 1 w.r.t. ϕi ’s enforces the nodes ϕi ’s within the same community\n",
      "to get closer to the corresponding community center ψk . That is,\n",
      "two nodes sharing a community are likely to have similar embedding. Compared with the first- and second-order proximity, this\n",
      "design enforces community-aware high-order proximity on node\n",
      "embedding, which is useful for community detection and embedding later. For example, in Fig. 1(a), node 3 and node 10 are directly\n",
      "linked, but they tend to belong to two different communities according to [35]. Therefore, by only preserving first-order proximity,\n",
      "we may not tell their community membership’s difference well. For\n",
      "another example, node 9 and node 10 share a number of one-hop\n",
      "and two-hop neighbors, but compared with node 10, node 9 tend to\n",
      "be closer to community led by node 1 according to [35]. Therefore,\n",
      "by only preserving second-order proximity, we may not tell their\n",
      "community membership’s difference well either.\n",
      "Based on the closed loop, we optimize community detection,\n",
      "community embedding and node embedding together. We have\n",
      "three types of proximity to consider for node embedding, including\n",
      "first-, second- and high-order proximity. In general, there are two\n",
      "approaches to combine different types of proximity for node embedding: (1) “concatenation”, e.g., LINE first separately optimizes\n",
      "_O1 and O2, then it concatenates the two resulting embedding for_\n",
      "each node into a long vector as the final output; (2) “unification”,\n",
      "e.g., SDNE [29] learns a single node embedding for each node to\n",
      "preserve both first- and second-order proximity at the same time.\n",
      "In this paper, to encourage the node embedding to unify multiple\n",
      "types of proximity, we adopt the unification approach, and leave\n",
      "the other approach as future work. Consequently, based on Eq. 1,\n",
      "we define the objective function for community detection and embedding, as well as enforcing the high-order proximity for node\n",
      "embedding as\n",
      "\n",
      "_O3 = −_ _K[β]_ �i|V= |1 [log][ �]k[K]=1 _[π][ik]_ [N(][ϕ][i][ |][ψ][k] [,][ Σ][k] [)][,] (6)\n",
      "\n",
      "where β ≥ 0 is a trade-off parameter. Denote Φ = {ϕi }, Φ[′] = {ϕi[′][}][,]\n",
      "Π = {πik }, Ψ = {ψk[′] [}][ and][ Σ][ =][ {][Σ][k][ }][ for][ i][ =][ 1][, ...,][ |][V][ |][ and][ k][ =]\n",
      "1, ..., _K._\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Then, we unify the first- and second-order proximity for node\n",
      "embedding. The ultimate objective function for ComE is\n",
      "\n",
      "L(Φ, Φ[′], Π, Ψ, Σ) = O1(Φ) + O2(Φ, Φ[′]) + O3(Φ, Π, Ψ, Σ) (7)\n",
      "\n",
      "Our final optimization problem becomes:\n",
      "\n",
      "(Φ[∗], Φ[′∗], Π[∗], Ψ[∗], Σ[∗]) ← arg min L(Φ, Φ[′], Π, Ψ, Σ) (8)\n",
      "∀k,diaд(Σk )>0\n",
      "\n",
      "where diaд(Σk ) returns the diagonal entries of Σk . We particularly\n",
      "introduce a constraint of diaд(Σk ) > 0 for each k ∈{1, ..., _K_ } to\n",
      "avoid the singularity issue of optimizing L. Similar to GMM [3],\n",
      "there exists degenerated solutions for optimizing L without any\n",
      "constraint. That is, when a Gaussian component collapses to a\n",
      "single point, the diaд(Σk ) becomes zero, which makes O3 become\n",
      "negative infinity.\n",
      "\n",
      "### 4 INFERENCE\n",
      "\n",
      "Since our objective in Eq. 8 can be seen as consisted of node embedding and community embedding, we decompose the optimization\n",
      "into two parts, and take an iterative approach to solve it. Specifically,\n",
      "we consider iteratively optimizing (Π, Ψ, Σ) with a constrained\n",
      "minimization given (Φ, Φ[′]), and optimizing (Φ, Φ[′]) with an unconstrained minimization given (Π, Ψ, Σ). Empirically, this iterative\n",
      "optimization algorithm converges quickly with a reasonable initialization, e.g., we initialize (Φ, Φ[′]) by DeepWalk results in our\n",
      "experiments. We report the convergence in Sec. 5.4. Next we detail\n",
      "this iterative optimization.\n",
      "\n",
      "**Fix (Φ, Φ[′]), optimize (Π, Ψ, Σ). In this case, Eq. 8 is simplified as**\n",
      "inferring a, with the constraints of diaд(Σk ) > 0 for each k ∈\n",
      "{1, ..., _K_ }. To solve this constrained optimization, we adopt the\n",
      "approach as suggested by [3], i.e., we use expectation maximization\n",
      "(EM) algorithm [9] to infer (Π, Ψ, Σ), and meet the constraint via\n",
      "suitable heuristics of randomly resetting Σk > 0 and ψk ∈ R[d]\n",
      "\n",
      "whenever a diaд(Σk ) starts to have zero. Particularly, by EM, we\n",
      "can iteratively update the (Π, Ψ, Σ) by\n",
      "\n",
      "_πik =_ _[N]|V[k] |_ [,] (9)\n",
      "\n",
      "_ψk =_ _N1k_ �i|V= |1 _[γ][ik]_ _[ϕ][i]_ [,] (10)\n",
      "\n",
      "Σk = _N1k_ �i|V= |1 _[γ][ik]_ [(][ϕ][i][ −] _[ψ][k]_ [)(][ϕ][i][ −] _[ψ][k]_ [)][T][,] (11)\n",
      "\n",
      "\n",
      "**Algorithm 1 Inference algorithm for ComE**\n",
      "\n",
      "**Require: graph G = (V**, E), #(community) K, #(paths per node) γ,\n",
      "walk length ℓ, context size ζ, embedding dimension d, negative\n",
      "context size m, parameters (α, β).\n",
      "**Ensure: node embedding Φ, context embedding Φ[′], community**\n",
      "assignment Π, community embedding (Ψ, Σ).\n",
      "\n",
      "1: P ← SamplePath(G, ℓ);\n",
      "\n",
      "2: Initialize Φ and Φ[′] by DeepWalk [20] with P;\n",
      "\n",
      "3: for iter = 1 : T1 do\n",
      "\n",
      "4: **for subiter = 1 : T2 do**\n",
      "\n",
      "5: Update πik, ψk and Σk by Eq. 9, Eq. 10 and Eq. 11;\n",
      "\n",
      "6: **for k = 1, ...,** _K do_\n",
      "\n",
      "7: **if there exists zero in diaд(Σk** ) then\n",
      "\n",
      "8: Randomly reset Σk > 0 and ψk ∈ R[d] ;\n",
      "\n",
      "9: **for all edge (i, j) ∈** _E do_\n",
      "\n",
      "10: SGD on ϕi and ϕ _j by Eq. 14;_\n",
      "\n",
      "11: **for all path p ∈P do**\n",
      "\n",
      "12: **for all vi in path p do**\n",
      "\n",
      "13: SGD on ϕi by Eq. 15;\n",
      "\n",
      "14: SGD on its context ϕ _j[′][’s within][ ζ][ hops by Eq. 17;]_\n",
      "\n",
      "15: **for all node vi ∈** _V do_\n",
      "\n",
      "16: SGD on ϕi by Eq. 16;\n",
      "\n",
      "It is easy to prove that O3[′][(][Φ][|][Π][,][ Ψ][,][ Σ][) ≥] _[O][3][(][Φ][|][Π][,][ Ψ][,][ Σ][)][ due to the]_\n",
      "following log-concavity\n",
      "� |V | �K\n",
      "\n",
      "_i=1_ [log] _k=1_ _[π][ik]_ [N(][ϕ][i][ |][ψ][k] [,][ Σ][k] [) ≥]\n",
      "\n",
      "(13)\n",
      "\n",
      "� |V | �K\n",
      "\n",
      "_i=1_ _k=1_ [log][ π][ik] [N(][ϕ][i][ |][ψ][k] [,][ Σ][k] [)]\n",
      "\n",
      "As a result, we define\n",
      "\n",
      "\n",
      "L [′](Φ, Φ[′]|Π, Ψ, Σ) = O1(Φ) + O2(Φ, Φ[′]) + O3[′][(][Φ][|][Π][,][ Ψ][,][ Σ][)]\n",
      "\n",
      "and, thus, L [′](Φ, Φ[′]|Π, Ψ, Σ) ≥L(Φ, Φ[′]|Π, Ψ, Σ). We optimize L [′](Φ, Φ[′])\n",
      "by stochastic gradient descent (SGD) [17]. For each vi ∈ _V, we have_\n",
      "\n",
      "∂∂Oϕ _i1_ [=][ −] [�](i, _j)∈E_ _[σ]_ [(−][ϕ][T]j _[ϕ][i]_ [)][ϕ] _[j]_ [,] (14)\n",
      "\n",
      "∂∂Oϕ _i2_ [=][ −] _[α][ �]vj ∈Ci_ �σ (−ϕ _j[′]T ϕi_ )ϕ _j′_\n",
      "\n",
      "+ [�][m]t =1 [E][v]l [∼][P]n [(][v]l [)][[][σ] [(][ϕ]l[′]T ϕi )(−ϕl′[)]]�, (15)\n",
      "\n",
      "\n",
      "where γik = �kK[′]=π1ik[π] N([ik][′][ N(]ϕ _i |[ϕ]ψ[i][ |]k[ψ],_ Σ[k]k[′] )[,] [Σ][k][′] [)][ and][ N][k][ =][ �]i[|][V]=[ |]1 _[γ][ik]_ [. It is worth]\n",
      "\n",
      "noting that, in practice if (Φ, Φ[′]) are initialized reasonably (e.g., by\n",
      "DeepWalk in our experiments), the constraints of diaд(Σk ) > 0 are\n",
      "easily satisfied, thus the inference of (Π, Ψ, Σ) can converge quickly.\n",
      "\n",
      "**Fix (Π, Ψ, Σ), optimize (Φ, Φ[′]). In this case, Eq. 8 is simplified as**\n",
      "an unconstrained optimization over the node embedding with three\n",
      "types of proximity. Due to the summation within the logarithm\n",
      "term of O3, it is inconvenient to compute the gradient of ϕi . Thus,\n",
      "we try to minimize an upper bound of L(Φ, Φ[′]|Φ, Ψ, Σ) instead.\n",
      "Specifically, we introduce\n",
      "\n",
      "_O3[′]_ [=][ −] _K[β]_ �i|V= |1 �kK=1 _[π][ik][ log][ N(][ϕ][i][ |][ψ][k]_ [,][ Σ][k] [)] (12)\n",
      "\n",
      "\n",
      "∂∂Oϕ _i3[′]_ [=][ β]K �kK=1 _[π][ik][ Σ]k[−][1][(][ϕ][i][ −]_ _[ψ][k]_ [)][.] (16)\n",
      "\n",
      "We also compute the gradient for context embedding as\n",
      "\n",
      "\n",
      "∂O2\n",
      "∂ϕ[′]j [=][ −][α][ �]vi ∈V\n",
      "\n",
      "\n",
      "�δ (vj ∈ _Ci_ )σ (−ϕ _j[′]T ϕi_ )ϕi\n",
      "\n",
      "\n",
      "+ [�][m]t =1 [E][v]l [∼][P]n [(][v]l [)][[][δ] [(][v][l][ =][ v][j] [)][σ] [(][ϕ]l[′]T ϕi )(−ϕi )]� (17)\n",
      "\n",
      "**Algorithm and complexity. We summarize the inference algo-**\n",
      "rithm of ComE in Alg. 1. In line 1, for each vi ∈ _V, we sample_\n",
      "_γ paths starting from vi with length ℓ_ on G. In line 2, we initialize (Φ, Φ[′]) by DeepWalk. In lines 4–8, we fix (Φ, Φ[′]) and optimize\n",
      "(Π, Ψ, Σ) for community detection and embedding. In lines 9–16,\n",
      "we fix (Π, Ψ, Σ) and optimize (Φ, Φ[′]) for node embedding.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "Particularly, we update node embedding by first-order proximity\n",
      "(lines 9–10), second-order proximity (lines 11–14) and communityaware high-order proximity (lines 15–16). We analyze the complexity of Alg. 1. Path sampling in line 1 takes O(|V |γ ℓ). Parameter initialization by DeepWalk in line 2 takes _O(|V |). Community detection_\n",
      "and embedding in line 5 takes O(|V |K). Checking constraint in lines\n",
      "6–8 takes O(K). Node embedding w.r.t. first-order proximity in lines\n",
      "9–10 takes O(|E|). Node embedding w.r.t. second-order proximity\n",
      "in lines 11–14 takes O(|V |γ ℓ). Node embedding w.r.t. communityaware high-order proximity in lines 15–16 takes O(|V |K). In total,\n",
      "the complexity is _O(|V |γ_ ℓ+|V |+T1×(T2|V |K+K+|E|+|V |γ ℓ+|V |K)),\n",
      "which is linear to the graph size (i.e., |V | and |E|). Thus, Alg. 1 is\n",
      "efficient.\n",
      "\n",
      "### 5 EXPERIMENTS\n",
      "\n",
      "As community embedding is useful for visualizing communities\n",
      "in a low-dimensional space, as well as helping both community\n",
      "detection and node embedding, we design three evaluation tasks\n",
      "for experiments: graph visualization, community detection and node\n",
      "_classification. Besides, we also empirically study the model conver-_\n",
      "gence and parameter sensitivity in this section. We provide the\n",
      "code used during the experiments at the following link[3].\n",
      "\n",
      "**Datasets. We use five public graph datasets for evaluation. These**\n",
      "graphs are of various types, ranging from social networks to word\n",
      "co-occurrence network and academic paper citation network.\n",
      "\n",
      "- BlogCatalog[4] is a social network for users to publish blogs. In\n",
      "this dataset, each node is a BlogCatalog user and each edge is a\n",
      "friendship connection. Each node has multiple labels, indicating\n",
      "the topics of the user’s blog topics.\n",
      "\n",
      "- Flickr[5] is social network for users to share images and videos. In\n",
      "this dataset, each node is a Flickr user and each edge is a friendship\n",
      "connection. Each node has a label, indicating the user’s interest\n",
      "group such as \"Sea Explorer\".\n",
      "\n",
      "- Wikipedia[6] is a co-occurrence network of words appearing in the\n",
      "fist million bytes of the Wikipedia dump. In this dataset, each node\n",
      "is a word and each edge is a word co-occurrence relationship. Each\n",
      "node has a label, indicating the word’s part-of-speech tag.\n",
      "\n",
      "- DBLP[7] is an academic paper citation network built upon the DBLP\n",
      "repository. We extracted the papers from 19 selected conferences\n",
      "from five areas, as shown in Tab. 4. In this dataset, each node is a\n",
      "paper and each edge is a citation. Each node has a label, indicating\n",
      "one of the five areas for its paper’s conference venue.\n",
      "\n",
      "- Karate Club is a social network of a university karate club [35].\n",
      "We summarize the statistics and the evaluation tasks for each\n",
      "dataset in Tab. 3.\n",
      "\n",
      "**Evaluation metrics. In community detection, we use both con-**\n",
      "_ductance [14] and normalized mutual information (NMI) [27]. Con-_\n",
      "ductance is basically a ratio between the number of edges leaving\n",
      "a community and that within the community. NMI measures the\n",
      "closeness between the predicted communities with ground truth\n",
      "based on the node labels.\n",
      "\n",
      "[3https://github.com/andompesta/ComE.git](https://github.com/andompesta/ComE.git)\n",
      "[4http://socialcomputing.asu.edu/datasets/BlogCatalog3](http://socialcomputing.asu.edu/datasets/BlogCatalog3)\n",
      "[5http://socialcomputing.asu.edu/datasets/Flickr](http://socialcomputing.asu.edu/datasets/Flickr)\n",
      "[6http://snap.stanford.edu/node2vec/POS.mat](http://snap.stanford.edu/node2vec/POS.mat)\n",
      "[7https://aminer.org/billboard/aminernetwork](https://aminer.org/billboard/aminernetwork)\n",
      "\n",
      "\n",
      "**Table 3: Datasets used for evaluation. Task “d” denotes com-**\n",
      "**munity detection, task “c” denotes node classification, and**\n",
      "**task “v” denotes graph visualization.**\n",
      "\n",
      "#(node) #(edge) #(class) #(label)/node tasks\n",
      "BlogCatalog 10,312 333,983 39 ≥ 1 d + c\n",
      "Flickr 80,513 5,899,882 195 ≥ 1 d + c\n",
      "Wikipedia 4,777 184,812 40 ≥ 1 d + c\n",
      "DBLP 13,184 48,018 5 1 v + d + c\n",
      "Karate Club 34 78 2 ≥ 1 v\n",
      "\n",
      "**Table 4: DBLP dataset labels.**\n",
      "\n",
      "Conference Label\n",
      "EMNLP, ACL, CoNLL, COLING NLP\n",
      "CVPR, ICCV, ICIP, SIGGRAPH Computer Vision\n",
      "KDD, ICDM, CIKM, WSDM Data Mining\n",
      "SIGMOD, VLDB/PVLDB, ICDE Database\n",
      "INFOCOM, SIGCOM, MobiHoc, MobiCom Networking\n",
      "\n",
      "In node classification, we use micro-F1 and macro-F1 [20]. MicroF1 is the overall F1 w.r.t. all kinds of labels. Macro-F1 is the average\n",
      "of F1 scores w.r.t. each kind of label.\n",
      "\n",
      "**Baselines. We design baselines to back up our arguments in Sec. 1**\n",
      "\n",
      "that it is non-trivial to learn community embedding.\n",
      "\n",
      "- SF: We first design a straightforward approach that separates\n",
      "community detection and node embedding, later fits community\n",
      "embedding from the detection and node embedding results. We use\n",
      "Spectral Clustering [25] for community detection and use DeepWalk for node embedding; finally we use GMM to fit community\n",
      "embedding. Note that, since its node embedding is the same as\n",
      "DeepWalk, we will only evaluate SF in community detection and\n",
      "community visualization for the Karate dataset.\n",
      "Besides, we also consider the other approach that runs community detection on the node embedding results from the following\n",
      "state-of-the-art baselines and, then, runs GMM to detect communities and fit community embedding.\n",
      "\n",
      "- DeepWalk [20]: it models second-order proximity in the embedding process.\n",
      "\n",
      "- LINE [24]: it considers both first- and second-order proximity.\n",
      "\n",
      "- Node2Vec [12]: it extends DeepWalk by exploiting homophily and\n",
      "structural roles in embedding.\n",
      "\n",
      "- GraRep [5]: it models random walk based high-order proximity.\n",
      "\n",
      "- M-NMF [31]: it jointly models node and community embedding\n",
      "using non-negative matrix factorization.\n",
      "We compared our model with all the baselines on all the datasets,\n",
      "using the author-published codes. However, since baselines like\n",
      "GraRep, M-NMF often had to compute dense adjacency matrices\n",
      "we encounter unmanageable out-of-memory errors when running\n",
      "these baselines even with a 64GB-memory machine for the Flickr\n",
      "dataset. Similarly, Node2Vec is unfeasible on Flicker, since it has\n",
      "to compute and store the transition probability for each neighborhood of each node in order to perform non-uniform sampling from\n",
      "discrete distributions.\n",
      "\n",
      "|Col1|#(node)|#(edge)|#(class)|#(label)/node|tasks|\n",
      "|---|---|---|---|---|---|\n",
      "|BlogCatalog|10,312|333,983|39|≥1|d + c|\n",
      "|Flickr|80,513|5,899,882|195|≥1|d + c|\n",
      "|Wikipedia|4,777|184,812|40|≥1|d + c|\n",
      "|DBLP|13,184|48,018|5|1|v + d + c|\n",
      "|Karate Club|34|78|2|≥1|v|\n",
      "\n",
      "|Conference|Label|\n",
      "|---|---|\n",
      "|EMNLP, ACL, CoNLL, COLING|NLP|\n",
      "|CVPR, ICCV, ICIP, SIGGRAPH|Computer Vision|\n",
      "|KDD, ICDM, CIKM, WSDM|Data Mining|\n",
      "|SIGMOD, VLDB/PVLDB, ICDE|Database|\n",
      "|INFOCOM, SIGCOM, MobiHoc, MobiCom|Networking|\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**Parameters and environment. Our ComE has only two more**\n",
      "parameters (i.e., α and β) than DeepWalk. To obtain a fair comparison we follow the DeepWalk and Node2Vec works to set the\n",
      "parameters. Unless stated otherwise, for all the methods, we set\n",
      "the embedding dimension d = 128. For DeepWalk, Node2Vec and\n",
      "ComE, we set γ = ζ = 10, ℓ = 80 and m = 5. As our method, also\n",
      "Node2Vec present two more parameters that need to be tuned. For\n",
      "BlogCatalog and Wikipedia, we follow the work done in [12], where\n",
      "_p = 0.25 and q = 0.25 result as the best tuning for BlogCatalog;_\n",
      "while Wikipedia better perform with p = 4 and q = 1. We followed\n",
      "the same tuning procedure also for DBLP and we found out that,\n",
      "like BlogCatalog, p = 0.25 and q = 0.25 works at best. For M-NMF,\n",
      "we tune α and β in the range [0.1, 1, 5, 10] while keeping the other\n",
      "parameter fixed. The final setting is: (1) α = 0.1 and β = 5 for\n",
      "BlogCatalog and Wikipedia; (2) while for DBLP we used α = 10\n",
      "and β = 5. For all the datasets, we set K as the number of unique\n",
      "labels. We run experiments on Linux machines with eight 3.50GHz\n",
      "Intel Xeon(R) CPUs and 16GB memory.\n",
      "\n",
      "### 5.1 Graph Visualization\n",
      "\n",
      "We compare ComE with the baselines on both a small Karate Club\n",
      "graph and a bigger DBLP paper citation graph. We visualize the\n",
      "Karate Club graph in Fig. 3, based on the node and community\n",
      "embedding results of our baselines.\n",
      "\n",
      "(a) DeepWalk (b) LINE\n",
      "\n",
      "(c) Node2Vec (d) GraRep\n",
      "\n",
      "(e) M-NMF (f) SF\n",
      "**Figure 3: Graph visualization on the Karate Club graph.**\n",
      "\n",
      "\n",
      "(a) DeepWalk / SF (b) LINE\n",
      "\n",
      "(c) Node2Vec (d) GraRep\n",
      "\n",
      "(e) M-NMF (f) ComE\n",
      "**Figure 4: Graph visualization on the DBLP graph (better**\n",
      "**viewed in color). Different node colors indicate different**\n",
      "**communities; red is NLP, blue is Computer Vision, green is**\n",
      "**_Data Mining, yellow is Database and black is Networking_**\n",
      "\n",
      "As we can see, LINE does not present community structures,\n",
      "since it does not consider the community in its node embedding.\n",
      "DeepWalk, Node2Vec, GraRep and M-NMF tend to present two separate communities, which cannot identify those weak supporters\n",
      "for the communities (e.g., node 9), while SF detect noise communities due to the Spectral Clustering. In contrast, as shown in Fig. 1(b),\n",
      "our ComE can clearly identify such weak supporters thanks to its\n",
      "joint modeling of overlapping community detection, community\n",
      "embedding and node embedding.\n",
      "We visualize the DBLP paper citation graph in Fig. 4. We use the\n",
      "t-SNE toolkit [28] for graph visualization. Instead of plotting community embedding eclipses, in t-SNE we use different node colors\n",
      "to visualize different communities and see if the algorithms are able\n",
      "to correctly preserve the communities present in the graphs. Note\n",
      "that, although SF and DeepWalk have different way to generate\n",
      "the community embedding, they use the same node embedding,\n",
      "thus, they have the visualization results in Fig. 4(a). First of all, note\n",
      "that no method is able to separate the green and the yellow classes\n",
      "(Data Mining and Database), we believe that this is related to the\n",
      "dataset itself given the similarity of the two topics. Moreover, DeepWalk and Node2Vec generate representations with many overlaps\n",
      "among the colors and the overall embedding is quite similar. This\n",
      "could be possible because both the methodology model only the\n",
      "second-order proximity.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "(a) BlogCatalog (b) DBLP (c) Flickr (d) Wikipedia\n",
      "\n",
      "**Figure 5: Community detection results. The smaller conductance is, the better. The bigger NMI is, the better.**\n",
      "\n",
      "\n",
      "LINE presents a bit more cohesive visualization; probably this is\n",
      "due to its ability in preserving first and second order proximity. We\n",
      "also observe that, in GraRep’s results, green nodes are spread in\n",
      "different places and mixed with red and yellow nodes, meaning a\n",
      "lack of clear community structure in the embedding. This might be\n",
      "possible due to its ability in decomposing a high-order transition\n",
      "probability matrix, which could be dominate by Data Mining and\n",
      "Database nodes. On the other hand, it is able to correctly detect Networking and Computer vision community. M-NMF, instead, present\n",
      "blurred community structure. This could be related to the fact that\n",
      "M-NMF relay on the modularity matrix to learn the community\n",
      "embedding, which could be noise due to the overlap between the\n",
      "green and yellow nodes. This hypothesis is also supported by the\n",
      "karate results (Fig. 3(e)), in which M-NMF already demonstrate to\n",
      "struggle in modeling overlapping communities. Compared with\n",
      "all these baseline methods, our ComE can correctly detect three\n",
      "classes thanks to its joint modeling, but as for the other methods\n",
      "is not able to differentiate between green and yellow nodes which\n",
      "are clustered in mixed communities.\n",
      "\n",
      "### 5.2 Community Detection\n",
      "\n",
      "In community detection, our goal is to predict the most likely community assignment for each node. As an unsupervised task, we use\n",
      "the whole graph for learning embeddings and, hence, predicting\n",
      "communities for each node. Note that in BlogCatalog, Wikipedia\n",
      "and Flickr are multi-labels datasets, so we compute only the conductance for the Top 2 communities of each node since there is no\n",
      "clear way to calculate NMI for such a multi-label setting.\n",
      "During those experiments we set α = 0.1, β = 0.1 in ComE for\n",
      "all the datasets. As shown in Fig. 5, ComE is consistently better\n",
      "than the baselines in terms of both conductance and NMI. For the\n",
      "conductance ComE improves the best baselines by relatively 2.2%\n",
      "to 3.1% on BlogCatalog and Wikipedia, 4.4% and 3.2% in Flickr, and\n",
      "17.1% in DBLP. Under NMI metrics ComE achieve an improvement\n",
      "of 6.7%. These improvements suggest that, modeling community\n",
      "detection together with node embedding is better than solving\n",
      "them separately. The general poor performance of SF methods also\n",
      "support the hypothesis of having the closed loop model. Besides,\n",
      "we also observe that, on average, the graph embedding methods\n",
      "perform better than the others, which suggest the usefulness of\n",
      "consider graph embedding for community detection. In particular, Node2Vec happen to be the best baseline for both DBLP and\n",
      "BlogCatalog datasets, while in Wikipedia GraRep outperform all\n",
      "the others. Finally, we observe that, accordingly to the findings in\n",
      "Sec. 5.1, M-NMF struggle in modeling multi-label datasets.\n",
      "\n",
      "\n",
      "### 5.3 Node Classification\n",
      "\n",
      "In node classification, our goal is to categorize each node into one\n",
      "or more classes, depending on whether it is a single-label or multilabel setting. We follow [20] to first train graph embedding on the\n",
      "whole graph, then randomly split 10% (BlogCatalog, Wikipedia and\n",
      "DBLP) and 90% (Flickr) of nodes as test data, respectively. We use\n",
      "the remaining nodes, together with their labels, to train a classifier\n",
      "by LibSVM (c = 1 for all the methods) [6]. We repeat 10 times and\n",
      "report the average results.\n",
      "We compare ComE with all the baselines in terms of node classification. We set α = 0.1, β = 0.01 for all the datasets except for\n",
      "DBLP where we kept α = 0.1 and β = 0.1. We vary the number\n",
      "of training data to build the classifiers for each method. As shown\n",
      "in Tab. 5, ComE is generally better than the baselines in terms of\n",
      "both macro-F1 and micro-F1. In particular, ComE improves the best\n",
      "baselines by relatively 0.8% to 22.6% (macro-F1) and 0.71% to 48%\n",
      "(micro-F1), when using 80% (BlogCatalog, Wikipedia and DBLP)\n",
      "and 26.9% (macro) 1.5% (micro) when using 8% (Flickr) of labeled\n",
      "nodes for training. Our student t-tests show that all the above relative improvements are significant over the 10 data splits, with\n",
      "one-tailed p-values always less than 0.01. It is interesting to see\n",
      "ComE improves the baselines on node classification, since it is unsupervised and it does not directly optimize the classification loss.\n",
      "This implies the high-order proximity from community embedding\n",
      "does contribute to node embedding. In addition, we also make some\n",
      "interesting observations from Tab. 5.\n",
      "Firstly, in Wikipedia, GraRep is better than our ComE when\n",
      "using less than 50% labeled nodes in training under the MicroF1 score. A possible reason is that, Wikipedia contains a much\n",
      "smaller number of nodes than the rest of the datasets, leading to\n",
      "a comparatively smaller set of sampled paths. On the other hand,\n",
      "GraRep used the transition probability matrix, which could contain\n",
      "more information than the sample path, to learn higher-order of\n",
      "proximity. This provides supplemental information to train the\n",
      "classifiers with limited training labels. However, as more labeled\n",
      "data is available, this advantage of GraRep becomes smaller, and\n",
      "our ComE starts to outperform.\n",
      "Secondly, it is possible to notice how in BlogCatalog, GraRep\n",
      "is the best baseline; meanwhile Node2Vec outperforms the other\n",
      "baselines in DBLP. This maybe because the number of edges in\n",
      "DBLP is much fewer than those of the other datasets. In a graph\n",
      "with low average degree, it is easier for Node2Vec learning which\n",
      "are the good neighborhood to explore. Moreover, respect tho all\n",
      "the other datasets, in Dblp the random walk based methods works\n",
      "relatively better than the factorization based methods.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "**Table 5: Node classification results.**\n",
      "**BlogCatalog** **Flickr**\n",
      "% Labels 10% 20% 30% 40% 50% 60% 70% 80% 90% 1% 2% 3% 4% 5% 6% 7% 8% 9%\n",
      "\n",
      "ComE 19.3 **22.4** **23.5** **24.8** **25.1** **25.4** **25.7** **26.2** **26.4** **3.5** **5.3** **10.1** **15.4** **18.2** **19.0** **19.3** **19.7** **20.0**\n",
      "DeepWalk/SF 17.2 18.9 19.9 20.6 20.9 21.4 21.5 21.5 21.5 1.9 3.1 7.9 11.6 13.8 14.3 15.2 15.5 15.7\n",
      "Line 7.6 8.6 9.5 10.0 10.2 10.8 10.8 10.9 10.9 1.6 2.5 6.0 8.7 10.2 10.5 11.0 11.2 11.2\n",
      "Node2Vec 18.3 20.5 21.9 22.8 23.1 23.4 23.6 23.6 23.6 - - - - - - - - GraRep **19.6** 21.0 22.3 22.8 22.9 23.3 23.5 23.6 24.0 - - - - - - - - M-NMF 12.8 13.7 14.7 15.2 15.3 15.3 15.6 15.7 15.8 - - - - - - - - \n",
      "ComE (ours) 30.1 **35.9** **37.3** **39.6** **40.3** **40.7** **41.0** **41.4** **42.4** **10.2** **12.2** **18.7** **29.1** **34.6** **35.1** **35.5** **35.9** **36.1**\n",
      "DeepWalk/SF 27.1 29.8 33.2 35.4 35.9 37.9 38.0 38.3 39.0 6.1 7.9 17.4 27.1 32.9 33.2 35.1 35.3 35.5\n",
      "Line 18.5 21.5 25.2 27.2 27.6 29.5 29.8 30.2 30.5 3.9 5.5 14.7 24.5 30.3 30.6 32.3 32.5 32.6\n",
      "Node2Vec 27.8 31.0 34.7 36.9 37.4 39.4 39.6 39.9 40.5 - - - - - - - GraRep **30.4** 32.9 36.3 38.2 38.6 40.2 40.7 40.9 42.0 - - - - - - - M-NMF 23.2 25.7 29.0 31.0 31.5 33.2 33.4 33.8 34.4 - - - - - - - - \n",
      "**Wikipedia** **DBLP**\n",
      "% Labels 10% 20% 30% 40% 50% 60% 70% 80% 90% 10% 20% 30% 40% 50% 60% 70% 80% 90%\n",
      "\n",
      "ComE **5.5** **5.5** **4.9** **4.9** **5.6** **6.9** **8.6** **10.6** **11.2** **91.1** **91.6** **91.8** **92.0** **92.1** **92.2** **92.2** **92.2** **92.4**\n",
      "DeepWalk/SF 2.3 2.4 2.6 2.7 3.9 4.4 5.1 6.2 10.9 89.7 90.5 90.8 91.0 91.1 91.2 91.2 91.2 91.4\n",
      "Line 2.1 2.3 2.4 2.5 2.8 3.3 3.9 5.3 8.9 89.2 89.8 90.1 90.2 90.3 90.3 90.4 90.4 90.7\n",
      "Node2Vec 2.9 2.9 3.1 3.3 4.1 5.8 7.7 8.3 9.5 90.0 90.7 91.0 91.2 91.4 91.4 91.5 91.5 91.7\n",
      "GraRep 4.5 4.8 4.8 4.8 4.8 5.6 6.3 8.1 11.0 89.9 90.1 90.3 90.4 90.5 90.5 90.5 90.6 90.8\n",
      "M-NMF 1.7 1.7 1.7 1.7 1.8 2.3 3.0 3.9 5.3 88.3 88.8 89.2 89.3 89.4 89.5 89.5 89.6 89.8\n",
      "\n",
      "ComE **25.4** 25.3 24.6 24.3 **27.7** **31.2** **38.3** **49.5** **50.0** **91.6** **92.0** **92.2** **92.4** **92.5** **92.6** **92.6** **92.6** **92.8**\n",
      "DeepWalk/SF 20.0 20.2 20.4 20.9 24.1 25.1 26.8 31.1 45.3 90.3 90.9 91.2 91.4 91.6 91.6 91.6 91.6 91.8\n",
      "Line 21.5 22.2 22.4 22.5 23.0 24.4 26.1 30.5 44.2 89.9 90.5 90.7 90.8 90.9 91.0 91.0 91.0 91.4\n",
      "Node2Vec 20.7 21.2 21.3 21.4 25.6 29.7 37.4 40.6 47.3 90.6 91.2 91.5 91.7 91.9 91.9 91.9 92.0 92.2\n",
      "GraRep 24.6 **25.4** **25.8** **25.8** 26.1 27.1 29.1 33.4 46.0 90.4 90.6 90.8 90.9 91.0 91.1 91.1 91.1 91.4\n",
      "M-NMF 19.6 20.4 21.2 21.6 22.5 23.9 25.7 30.5 45.1 89.1 89.5 89.9 90.0 90.1 90.2 90.2 90.3 90.5\n",
      "\n",
      "(a) Conductance (b) NMI (c) Micro-F1 (d) Macro-F1\n",
      "\n",
      "**Figure 6: Impact of the parameters α and β. Note that NMI has only DBLP since is the only single-label dataset.**\n",
      "\n",
      "|BlogCatalog|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Flickr|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "||% Labels|10%|20%|30%|40%|50%|60%|70%|80%|90%|1%|2%|3%|4%|5%|6%|7%|8%|9%|\n",
      "|Macro-F1 (%)|ComE DeepWalk/SF Line Node2Vec GraRep M-NMF|19.3 17.2 7.6 18.3 19.6 12.8|22.4 18.9 8.6 20.5 21.0 13.7|23.5 19.9 9.5 21.9 22.3 14.7|24.8 20.6 10.0 22.8 22.8 15.2|25.1 20.9 10.2 23.1 22.9 15.3|25.4 21.4 10.8 23.4 23.3 15.3|25.7 21.5 10.8 23.6 23.5 15.6|26.2 21.5 10.9 23.6 23.6 15.7|26.4 21.5 10.9 23.6 24.0 15.8|3.5 1.9 1.6 - - -|5.3 3.1 2.5 - - -|10.1 7.9 6.0 - - -|15.4 11.6 8.7 - - -|18.2 13.8 10.2 - - -|19.0 14.3 10.5 - - -|19.3 15.2 11.0 - - -|19.7 15.5 11.2 - - -|20.0 15.7 11.2 - - -|\n",
      "|(%) Micro-F1|ComE (ours) DeepWalk/SF Line Node2Vec GraRep M-NMF|30.1 27.1 18.5 27.8 30.4 23.2|35.9 29.8 21.5 31.0 32.9 25.7|37.3 33.2 25.2 34.7 36.3 29.0|39.6 35.4 27.2 36.9 38.2 31.0|40.3 35.9 27.6 37.4 38.6 31.5|40.7 37.9 29.5 39.4 40.2 33.2|41.0 38.0 29.8 39.6 40.7 33.4|41.4 38.3 30.2 39.9 40.9 33.8|42.4 39.0 30.5 40.5 42.0 34.4|10.2 6.1 3.9 - - -|12.2 7.9 5.5 - - -|18.7 17.4 14.7 - - -|29.1 27.1 24.5 - - -|34.6 32.9 30.3 - - -|35.1 33.2 30.6 - - -|35.5 35.1 32.3 - - -|35.9 35.3 32.5 - - -|36.1 35.5 32.6 -|\n",
      "\n",
      "|Wikipedia|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|DBLP|Col13|Col14|Col15|Col16|Col17|Col18|Col19|Col20|\n",
      "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
      "||% Labels|10%|20%|30%|40%|50%|60%|70%|80%|90%|10%|20%|30%|40%|50%|60%|70%|80%|90%|\n",
      "|Macro-F1 (%)|ComE DeepWalk/SF Line Node2Vec GraRep M-NMF|5.5 2.3 2.1 2.9 4.5 1.7|5.5 2.4 2.3 2.9 4.8 1.7|4.9 2.6 2.4 3.1 4.8 1.7|4.9 2.7 2.5 3.3 4.8 1.7|5.6 3.9 2.8 4.1 4.8 1.8|6.9 4.4 3.3 5.8 5.6 2.3|8.6 5.1 3.9 7.7 6.3 3.0|10.6 6.2 5.3 8.3 8.1 3.9|11.2 10.9 8.9 9.5 11.0 5.3|91.1 89.7 89.2 90.0 89.9 88.3|91.6 90.5 89.8 90.7 90.1 88.8|91.8 90.8 90.1 91.0 90.3 89.2|92.0 91.0 90.2 91.2 90.4 89.3|92.1 91.1 90.3 91.4 90.5 89.4|92.2 91.2 90.3 91.4 90.5 89.5|92.2 91.2 90.4 91.5 90.5 89.5|92.2 91.2 90.4 91.5 90.6 89.6|92.4 91.4 90.7 91.7 90.8 89.8|\n",
      "|Micro-F1 (%)|ComE DeepWalk/SF Line Node2Vec GraRep M-NMF|25.4 20.0 21.5 20.7 24.6 19.6|25.3 20.2 22.2 21.2 25.4 20.4|24.6 20.4 22.4 21.3 25.8 21.2|24.3 20.9 22.5 21.4 25.8 21.6|27.7 24.1 23.0 25.6 26.1 22.5|31.2 25.1 24.4 29.7 27.1 23.9|38.3 26.8 26.1 37.4 29.1 25.7|49.5 31.1 30.5 40.6 33.4 30.5|50.0 45.3 44.2 47.3 46.0 45.1|91.6 90.3 89.9 90.6 90.4 89.1|92.0 90.9 90.5 91.2 90.6 89.5|92.2 91.2 90.7 91.5 90.8 89.9|92.4 91.4 90.8 91.7 90.9 90.0|92.5 91.6 90.9 91.9 91.0 90.1|92.6 91.6 91.0 91.9 91.1 90.2|92.6 91.6 91.0 91.9 91.1 90.2|92.6 91.6 91.0 92.0 91.1 90.3|92.8 91.8 91.4 92.2 91.4 90.5|\n",
      "\n",
      "\n",
      "This suggests that the average degree of the graph can impact\n",
      "the performance due to the length and the windows size limitation\n",
      "of the random walk methods. Besides this intuition, a deeper investigation is needed to better understand the limitation and the\n",
      "advantages of the random walks methods.\n",
      "Thirdly, it is interestingly to notice how in node classification\n",
      "the best baseline is the opposite respect community detection. In\n",
      "Sec. 5.2 Node2Vec was the best baseline for BlogCatalog and DBLP,\n",
      "while in node classification task GraRep outperform Node2Vec.\n",
      "This suggest that focusing on community level as higher order of\n",
      "proximity is more valuable than learning the graph structure form\n",
      "the transition probability matrix, since lead to better performances\n",
      "in a wider tasks set.\n",
      "\n",
      "\n",
      "### 5.4 Model Study\n",
      "\n",
      "We tune the model parameters α (trade-off parameter for context\n",
      "embedding) and β (trade-off parameter for community embedding)\n",
      "in Fig. 6. In each figure, we tune one parameters α and β within\n",
      "the range of [0.001, 1] while fixing the other parameter as 0.1. In\n",
      "general, the model performance is quite robust when α and β are\n",
      "within the range of [0.001, 1]. Specifically, α = 0.1 gives the best\n",
      "trade off for the second-order proximity in the objective function as\n",
      "it provides the best community prediction results in Wikipedia and\n",
      "DBLP, and the best node classification results in BlogCatalog and\n",
      "Flickr. The tuning of β is not as sensitive as α, especially in terms\n",
      "of node classification task. However, as indicated by community\n",
      "detection performance in BlogCatalog, β = 0.1 is the best trade-off\n",
      "for community embedding.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "(a) Convergence (b) Efficiency\n",
      "\n",
      "**Figure 7: Model’s convergence and efficiency**\n",
      "\n",
      "We further validate the convergence and efficiency of ComE in\n",
      "Fig. 7. We record the value of the loss function (Eq. 7) at the end of\n",
      "every iteration (line 18 in Alg. 1) to show the convergence of our\n",
      "proposed ComE. Note that we normalize the loss value by |V | for\n",
      "different datasets so as to better illustrate them in one figure. As\n",
      "shown in Fig. 7(a), the loss of ComE converges quickly within 2–3\n",
      "iterations. To demonstrate the efficiency of ComE, we test it on all\n",
      "the four datasets at different scales.\n",
      "For each dataset, we generate three subsets in which we keep 25%,\n",
      "50% and 75% of the total number of edges and nodes, respectively.\n",
      "Note that, to speed up the computation time of those experiments\n",
      "we set d = 2 and ζ = 5. The diagram in Fig. 7(b) shows the processing time of ComE (line 2 – line 19 in Alg. 1) in different datasets.\n",
      "Clearly, the processing time of ComE is linear to the graph size (i.e.,\n",
      "|V | and |E|). This validates our complexity analysis at the end of\n",
      "Sec. 4.\n",
      "\n",
      "### 6 CONCLUSION\n",
      "\n",
      "In this paper, we studied the important, yet largely under-explored\n",
      "problem of embedding communities on graphs. We developed a\n",
      "closed loop among community embedding, community detection\n",
      "and node embedding. Therefore, we tried to jointly optimize all\n",
      "these three tasks, in order to allow them to reinforce each other.\n",
      "We then developed a scalable inference algorithm, which only requires a complexity of O(|V | + |E|). We evaluated our model on multiple real-world datasets and showed that our model outperforms\n",
      "the state-of-the-art baselines by at least 6.6% (NMI) and 2.2%–16.9%\n",
      "(conductance) in community detection, 0.8%–26.9% (macro-F1) and\n",
      "0.71%–48% (micro-F1) in node classification. It also improved the\n",
      "baselines in the task of graph visualization.\n",
      "\n",
      "### ACKNOWLEDGEMENTS\n",
      "\n",
      "We thank the support of: National Natural Science Foundation of\n",
      "China (No. 61502418), Research Grant for Human-centered Cyberphysical Systems Programme at Advanced Digital Sciences Center\n",
      "from Singapore A*STAR, National Science Foundation IIS 16-19302\n",
      "and CSD-Centro Sistemi Direzionali.\n",
      "\n",
      "### REFERENCES\n",
      "\n",
      "[1] Shiyu Chang andRSM:Dai2016 Wei Han, Jiliang Tang, Guo-Jun Qi, Charu C.\n",
      "Aggarwal, and Thomas S. Huang. 2015. Heterogeneous Network Embedding via\n",
      "Deep Architectures. In KDD. 119–128.\n",
      "\n",
      "[2] Mikhail Belkin and Partha Niyogi. 2001. Laplacian Eigenmaps and Spectral\n",
      "Techniques for Embedding and Clustering. In NIPS. 585–591.\n",
      "\n",
      "[3] Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Infor_mation Science and Statistics). Springer-Verlag New York, Inc., NJ, USA._\n",
      "\n",
      "\n",
      "\n",
      "[4] HongYun Cai, Vincent W. Zheng, Fanwei Zhu, Kevin Chen-Chuan Chang, and\n",
      "Zi Huang. 2017. From Community Detection to Community Profiling. PVLDB\n",
      "10, 7 (2017), 817–828.\n",
      "\n",
      "[5] Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2015. GraRep: Learning Graph Representations with Global Structural Information. In CIKM. 891–900.\n",
      "\n",
      "[6] Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A Library for Support\n",
      "Vector Machines. ACM Trans. Intell. Syst. Technol. 2, 3 (May 2011), 27:1–27:27.\n",
      "\n",
      "[7] Trevor F. Cox and M.A.A. Cox. 2000. Multidimensional Scaling, Second Edition (2\n",
      "ed.). Chapman and Hall/CRC.\n",
      "\n",
      "[8] Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative Embeddings of Latent\n",
      "Variable Models for Structured Data. In ICML. 2702–2711.\n",
      "\n",
      "[9] Arther P. Dempster, Nan M. Laird, and Donald B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical\n",
      "_Society. Series B (Methodological) 39, 1 (1977), 1–38._\n",
      "\n",
      "[10] Hanyin Fang, Fei Wu, Zhou Zhao, Xinyu Duan, Yueting Zhuang, and Martin\n",
      "Ester. 2016. Community-Based Question Answering via Heterogeneous Social\n",
      "Network Learning. In AAAI. 122–128.\n",
      "\n",
      "[11] Yuan Fang, Wenqing Lin, Vincent W. Zheng, Min Wu, Kevin Chen-Chuan Chang,\n",
      "and Xiaoli Li. 2016. Semantic proximity search on graphs with metagraph-based\n",
      "learning. In ICDE. 277–288.\n",
      "\n",
      "[12] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable Feature Learning for\n",
      "Networks. In KDD.\n",
      "\n",
      "[13] X. He, D. Cai, Y. Shao, H. Bao, and J. Han. 2011. Laplacian Regularized Gaussian\n",
      "Mixture Model for Data Clustering. TKDE 23, 9 (2011), 1406–1418.\n",
      "\n",
      "[14] Kyle Kloster and David F. Gleich. 2014. Heat Kernel Based Community Detection.\n",
      "In KDD. 1386–1395.\n",
      "\n",
      "[15] Mark Kozdoba and Shie Mannor. 2015. Community Detection via Measure Space\n",
      "Embedding. In NIPS. 2890–2898.\n",
      "\n",
      "[16] Zemin Liu, Vincent W. Zheng, Zhou Zhao, Fanwei Zhu, Kevin Chen-Chuan\n",
      "Chang, Minghui Wu, and Jing Ying. 2017. Semantic Proximity Search on Heterogeneous Graph by Proximity Embedding. In AAAI. 154–160.\n",
      "\n",
      "[17] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean.\n",
      "2013. Distributed Representations of Words and Phrases and their Compositionality. In NIPS. 3111–3119.\n",
      "\n",
      "[18] Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. 2016. Learning\n",
      "Convolutional Neural Networks for Graphs. In ICML. 2014–2023.\n",
      "\n",
      "[19] Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. 2016. Asymmetric Transitivity Preserving Graph Embedding. In KDD. 1105–1114.\n",
      "\n",
      "[20] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. DeepWalk: Online Learning of Social Representations. In KDD. 701–710.\n",
      "\n",
      "[21] Sam T. Roweis and Lawrence K. Saul. 2000. Nonlinear Dimensionality Reduction\n",
      "by Locally Linear Embedding. Science 290, 5500 (2000), 2323–2326.\n",
      "\n",
      "[22] Mrinmaya Sachan, Avinava Dubey, Shashank Srivastava, Eric P. Xing, and Eduard\n",
      "Hovy. 2014. Spatial Compactness Meets Topical Consistency: Jointly Modeling\n",
      "Links and Content for Community Detection. In WSDM. 503–512.\n",
      "\n",
      "[23] Yizhou Sun, Charu C. Aggarwal, and Jiawei Han. 2012. Relation Strength-aware\n",
      "Clustering of Heterogeneous Information Networks with Incomplete Attributes.\n",
      "_PVLDB 5, 5 (Jan. 2012), 394–405._\n",
      "\n",
      "[24] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\n",
      "2015. LINE: Large-scale Information Network Embedding. In WWW. 1067–1077.\n",
      "\n",
      "[25] Lei Tang and Huan Liu. 2011. Leveraging social media networks for classification.\n",
      "_Data Min. Knowl. Discov. 23, 3 (2011), 447–478._\n",
      "\n",
      "[26] Joshua B. Tenenbaum, Vin de Silva, and John C. Langford. 2000. A Global\n",
      "Geometric Framework for Nonlinear Dimensionality Reduction. Science 290,\n",
      "5500 (2000), 2319–2323.\n",
      "\n",
      "[27] Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. 2014. Learning Deep\n",
      "Representations for Graph Clustering. In AAAI. 1293–1299.\n",
      "\n",
      "[28] L.J.P. van der Maaten and G.E. Hinton. 2008. Visualizing High-Dimensional Data\n",
      "Using t-SNE. JMLR 9 (2008), 2579–2605.\n",
      "\n",
      "[29] Daixin Wang, Peng Cui, and Wenwu Zhu. 2016. Structural Deep Network Embedding. In KDD. 1225–1234.\n",
      "\n",
      "[30] Meng Wang, Chaokun Wang, Jeffrey Xu Yu, and Jun Zhang. 2015. Community\n",
      "Detection in Social Networks: An In-depth Benchmarking Study with a Procedureoriented Framework. PVLDB 8, 10 (June 2015), 998–1009.\n",
      "\n",
      "[31] Xiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. 2017.\n",
      "Community Preserving Network Embedding. In AAAI. 203–209.\n",
      "\n",
      "[32] Jierui Xie, Stephen Kelley, and Boleslaw K. Szymanski. 2013. Overlapping Community Detection in Networks: The State-of-the-art and Comparative Study.\n",
      "_ACM CSUR 45, 4 (2013), 43:1–43:35._\n",
      "\n",
      "[33] Ruobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and Maosong Sun. 2016. Representation Learning of Knowledge Graphs with Entity Descriptions. In AAAI.\n",
      "2659–2665.\n",
      "\n",
      "[34] Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong\n",
      "Zhang. 2016. Modularity Based Community Detection with Deep Learning. In\n",
      "_IJCAI. 2252–2258._\n",
      "\n",
      "[35] Wayne W. Zachary. 1977. An Information Flow Model for Conflict and Fission in\n",
      "Small Groups. Journal of Anthropological Research 33, 4 (1977), 452–473.\n",
      "\n",
      "\n",
      "-----\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pymupdf4llm \n",
    "\n",
    "pdf_file = \"/PaperSurvey/docs/prototype/Learning Community Embedding with Community Detection and Node Embedding on Graphs.pdf\"\n",
    "md_text = pymupdf4llm.to_markdown(pdf_file)\n",
    "\n",
    "print(md_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "debug: bool = False\n",
    "prompt_path = \"/PaperSurvey/prompts/v1.txt\"\n",
    "client = OpenAI()\n",
    "\n",
    "with open(prompt_path, encoding=\"UTF-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "prompt = prompt.replace(\"<<INPUT>>\", md_text)\n",
    "\n",
    "if debug:\n",
    "    print(prompt)\n",
    "    time.sleep(20)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10000,\n",
    "    top_p=0,\n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```output.md\n",
      "\n",
      "# abstract\n",
      "この論文では、グラフ埋め込みの重要でありながらほとんど未開拓の設定である、個々のノードではなくコミュニティを埋め込むことを研究しています。コミュニティ埋め込みは、グラフの視覚化などのコミュニティレベルのアプリケーションに有用であるだけでなく、コミュニティ検出やノード分類にも有益です。この埋め込みを学習するために、コミュニティ埋め込み、コミュニティ検出、ノード埋め込みの間に閉ループが存在するという洞察に基づいています。提案するフレームワークは、これら3つのタスクを同時に解決します。\n",
      "\n",
      "# 解決する課題\n",
      "## 既存研究の流れ（関連研究）\n",
      "従来のグラフ埋め込みは、個々のノードに焦点を当て、グラフ上で「近い」ノードが低次元空間で類似したベクトル表現を持つことを目指していました。しかし、コミュニティ全体を埋め込むことはあまり研究されていませんでした。\n",
      "\n",
      "## この研究が解決する課題・どう解決するのか\n",
      "解決する課題1: コミュニティ埋め込みの有用性を示す\n",
      "　→ コミュニティ埋め込みがグラフ視覚化やコミュニティ検出、ノード分類に有益であることを示す。\n",
      "解決する課題2: コミュニティ検出とノード埋め込みの相互作用を最適化\n",
      "　→ コミュニティ埋め込みを用いてノード埋め込みを最適化し、コミュニティ検出を改善する。\n",
      "解決する課題3: 効率的なコミュニティ埋め込みフレームワークの提案\n",
      "　→ コミュニティ検出、コミュニティ埋め込み、ノード埋め込みを同時に解決するフレームワークを提案。\n",
      "\n",
      "# 提案手法\n",
      "## 提案手法の直感的な説明\n",
      "コミュニティ埋め込みを多変量ガウス分布として定義し、ノード埋め込み結果からコミュニティ検出を強化するためにガウス混合モデルを使用します。\n",
      "\n",
      "## 提案手法詳細\n",
      "提案手法「ComE」は、コミュニティ検出、コミュニティ埋め込み、ノード埋め込みを同時に解決するフレームワークです。コミュニティ埋め込みを多変量ガウス分布として定義し、ノード埋め込み結果からコミュニティ検出を強化します。計算の複雑さはグラフサイズに対して線形です。\n",
      "\n",
      "# 実験\n",
      "## 実験設定\n",
      "5つの公開グラフデータセットを使用し、グラフ視覚化、コミュニティ検出、ノード分類の3つの評価タスクを設計しました。\n",
      "\n",
      "## 実験結果\n",
      "ComEは、グラフ視覚化、コミュニティ検出、ノード分類の各タスクで、最先端のベースラインを上回る結果を示しました。\n",
      "\n",
      "# 考察\n",
      "## なぜこの手法が優れているのか\n",
      "コミュニティ検出、コミュニティ埋め込み、ノード埋め込みを同時に最適化することで、各タスクが相互に強化されるため。\n",
      "\n",
      "## この手法が既存のものより優れている点・劣っている点\n",
      "優れている点: コミュニティ検出とノード分類の精度が向上し、グラフ視覚化が改善される。\n",
      "劣っている点: 特に記載なし。\n",
      "\n",
      "# 今後の発展\n",
      "論文には書かれていませんでした。\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=944, prompt_tokens=19449, total_tokens=20393, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "with open(prompt_path, encoding=\"UTF-8\") as f:\n",
    "    prompt = f.read()\n",
    "\n",
    "prompt = prompt.replace(\"<<INPUT>>\", md_text)\n",
    "\n",
    "response1 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt,\n",
    "    }\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=10000,\n",
    "    top_p=0,\n",
    "    frequency_penalty=None,\n",
    "    presence_penalty=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'``` output.md\\n\\n# abstract\\n本論文では、グラフ埋め込みの重要な設定である「コミュニティ埋め込み」に焦点を当て、個々のノードではなくコミュニティを埋め込むことの有用性を探求しています。コミュニティ埋め込みは、グラフの可視化やコミュニティ検出、ノード分類においても有益であることが示されています。著者たちは、コミュニティ埋め込み、コミュニティ検出、ノード埋め込みの間の閉じたループに基づいて、これらのタスクを共同で解決する新しいフレームワーク「ComE」を提案しています。このフレームワークは、複数の実世界のデータセットで評価され、グラフの可視化を改善し、コミュニティ検出やノード分類のさまざまなアプリケーションタスクにおいて最先端のベースラインを上回る結果を示しました。\\n\\n# 解決する課題\\n## 既存研究の流れ（関連研究）\\n- 伝統的なグラフ埋め込みは個々のノードに焦点を当てており、ノードのベクトル表現を生成することを目的としている。\\n- 既存の手法は、ノード埋め込みやコミュニティ検出を独立して行うため、最適な結果を得ることが難しい。\\n\\n## この研究が解決する課題・どう解決するのか\\n解決する課題1: コミュニティ埋め込みの重要性\\n\\u3000→ コミュニティ埋め込みは、コミュニティの可視化やノード分類において有益であることを示す。\\n\\n解決する課題2: コミュニティ検出とノード埋め込みの相互作用\\n\\u3000→ コミュニティ埋め込み、コミュニティ検出、ノード埋め込みの間の閉じたループを利用して、相互に最適化を行う。\\n\\n解決する課題3: 効率的なアルゴリズムの提案\\n\\u3000→ O(|V| + |E|)の計算量で動作するスケーラブルな推論アルゴリズムを開発。\\n\\n# 提案手法\\n## 提案手法の直感的な説明\\n提案手法「ComE」は、コミュニティ埋め込み、コミュニティ検出、ノード埋め込みを共同で最適化するフレームワークであり、これにより各タスクが相互に強化される。\\n\\n## 提案手法詳細\\n- コミュニティ埋め込みを多変量ガウス分布として定義し、ノード埋め込み結果からコミュニティを検出するためのガウス混合モデルを使用。\\n- ノード埋め込みは、第一、第二、高次の近接性を考慮して拡張される。\\n- 提案手法は、ノード埋め込みとコミュニティ埋め込みの最適化を交互に行う反復的なアプローチを採用。\\n\\n# 実験\\n## 実験設定\\n- 5つの公的グラフデータセット（BlogCatalog、Flickr、Wikipedia、DBLP、Karate Club）を使用。\\n- 各データセットに対して、コミュニティ検出、ノード分類、グラフ可視化のタスクを評価。\\n\\n## 実験結果\\n- ComEは、コミュニティ検出において最先端のベースラインを6.6%（NMI）および2.2%〜16.9%（導体）改善。\\n- ノード分類においても、0.8%〜26.9%（マクロF1）および0.71%〜48%（マイクロF1）の改善を示した。\\n\\n# 考察\\n## なぜこの手法が優れているのか\\n- コミュニティ埋め込み、コミュニティ検出、ノード埋め込みを共同で最適化することで、各タスクのパフォーマンスが向上する。\\n\\n## この手法が既存のものより優れている点・劣っている点\\n- 優れている点: コミュニティ構造を考慮した高次の近接性を導入することで、より意味のある埋め込みを生成。\\n- 劣っている点: 複雑なデータセットに対しては、他の手法と比較して計算コストが高くなる可能性がある。\\n\\n# 今後の発展\\n- 提案手法のさらなる改善や、異なるタイプのグラフデータに対する適用可能性を探求することが期待される。\\n- コミュニティ埋め込みの他の応用や、異なる埋め込み手法との統合についても研究が進められるべきである。\\n```'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionUsage(completion_tokens=944, prompt_tokens=19449, total_tokens=20393, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
