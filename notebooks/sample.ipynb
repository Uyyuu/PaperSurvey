{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/PaperSurvey\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "WORK_DIR = os.environ.get(\"WORK_DIR\")\n",
    "print(WORK_DIR)\n",
    "sys.path.append(WORK_DIR)\n",
    "\n",
    "from openai import OpenAI\n",
    "from notion_client import Client\n",
    "NOTION_TOKEN = os.environ.get(\"NOTION_TOKEN\")\n",
    "NOTION_DB_ID = os.environ.get(\"NOTION_DB_ID\")\n",
    "\n",
    "from src.llm.llm import LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LLM(base=\"openai\", model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /PaperSurvey/docs/prototype/BLT ByteLatentTransformer.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2[==                                      ] ( 2/27=[====                                    ] ( 3/27[=====                                   ] ( 4/2=[=======                                 ] ( 5/2[========                                ] ( 6/27=[==========                              ] ( 7/27[===========                             ] ( 8/2=[=============                           ] ( 9/2[==============                          ] (10/27=[================                        ] (11/27[=================                       ] (12/2=[===================                     ] (13/2[====================                    ] (14/27=[======================                  ] (15/27[=======================                 ] (16/2=[=========================               ] (17/2[==========================              ] (18/27=[============================            ] (19/27[=============================           ] (20/2=[===============================         ] (21/2[================================        ] (22/27=[==================================      ] (23/27[===================================     ] (24/2=[=====================================   ] (25/2[======================================  ] (26/27[========================================] (27/27]\n"
     ]
    }
   ],
   "source": [
    "prompt = open(\"/PaperSurvey/prompts/v1.txt\").read()\n",
    "import pymupdf4llm \n",
    "\n",
    "pdf_file = \"/PaperSurvey/docs/prototype/BLT ByteLatentTransformer.pdf\"\n",
    "title = \"BLT ByteLatentTransformer\"\n",
    "md_text = pymupdf4llm.to_markdown(pdf_file)\n",
    "\n",
    "prompt = prompt.replace(\"<<INPUT>>\", md_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user',\n",
       " 'content': [{'type': 'text',\n",
       "   'text': '# 指示書\\nこの論文を読みたいです。以下の制約を守り、要約をお願いします。\\n目的：論文の概要から詳細をつかみ、この論文をより詳しく読むべきか判断したい\\n対象読者：深層学習の基礎は知っている大学生\\n構成は、以下の例に従い、要約を生成するときは全ての内容を網羅した上で、この論文を理解するのに必要だと判断した部分を扱ってください。もし論文に書かれていないのであれば、「論文には書かれていませんでした」と出力すること。\\n全ては論文に書かれていることのみを使うこと。ハルシネーションは禁止です。\\n直訳ではなく、AIの文脈を考慮して文章を生成すること。\\n出力は文章をそのままではなく、マークダウンにして流れや構成要素をわかりやすくすること。\\n出力の長さは気にしないこと。途中で途切れても良いです。このタスクでは出力長制限よりも、私が与えたタスクを完璧にこなすことを何よりも優先すること。内容の抜け漏れは断じて許されません。\\n\\n# 出力フォーマットの例\\n```\\n# abstract\\n日本語訳\\n\\n# 解決する課題\\n## 既存研究の流れ（関連研究）\\n## この研究が解決する課題・どう解決するのか\\n解決する課題1\\n\\u3000→どう解決するか\\n解決する課題2\\n\\u3000→どう解決するか\\n解決する課題3\\n\\u3000→どう解決するか\\n（以下略）\\n\\n# 提案手法\\n## 提案手法の直感的な説明\\n## 提案手法詳細\\n提案手法の構成コンポーネントや、仕組みの詳細\\n\\n# 実験\\n## 実験設定\\n## 実験結果\\n\\n# 考察\\n## なぜこの手法が優れているのか\\n## この手法が既存のものより優れている点・劣っている点\\n\\n# 今後の発展\\n```\\n\\n# 論文\\n```\\n### 1 Introduction\\n\\nWe introduce the Byte Latent Transformer (BLT), a tokenizer-free architecture that learns from raw byte\\ndata and, for the first time, matches the performance of tokenization-based models at scale, with significant\\nimprovements in efficiency and robustness (§6). Existing large language models (llms) are trained almost\\nentirely end-to-end, except for tokenization—a heuristic pre-processing step that groups bytes into a static set\\nof tokens. Such tokens bias how a string is compressed, leading to shortcomings such as domain/modality\\nsensitivity (Dagan et al., 2024), sensitivity to input noise (§6), a lack of orthographic knowledge (Edman\\net al., 2024), and multilingual inequity (Petrov et al., 2024; Liang et al., 2023; Limisiewicz et al., 2024).\\n\\nTokenization has previously been essential because directly training llms on bytes is prohibitively costly\\nat scale due to long sequence lengths (Xue et al., 2022). Prior works mitigate this by employing more\\nefficient self-attention (El Boukkouri et al., 2020; Clark et al., 2022) or attention-free architectures (Wang\\net al., 2024) (§8). However, this primarily helps train small models. At scale, the computational cost of a\\nTransformer is dominated by large feed-forward network layers that run on every byte, not the cost of the\\nattention mechanism.\\n\\nTo efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches (§2)\\nand a new model architecture that mixes byte and patch information. Unlike tokenization, BLT has no fixed\\nvocabulary for patches. Arbitrary groups of bytes are mapped to latent patch representations via light-weight\\nlearned encoder and decoder modules. We show that this results in more efficient allocation of compute than\\ntokenization-based models.\\n\\nTokenization-based llms allocate the same amount of compute to every token. This trades efficiency for\\nperformance, since tokens are induced with compression heuristics that are not always correlated with the\\n\\n\\n-----\\n\\n1.000\\n\\n0.975\\n\\n\\n0.84\\n\\n0.82\\n\\n\\n0.950\\n\\n0.925\\n\\n\\n0.80\\n\\n0.78\\n\\n\\n0.900\\n\\n0.875\\n\\n\\n0.76\\n\\n0.74\\n\\n\\n0.850\\n\\n0.825\\n\\n\\nBPB vs Training Bytes at Fixed Inference FLOPs\\n\\nBLT Entropy ps=6 550M\\nBLT Entropy ps=8 760M\\nLLaMA 2 BPE 450M\\nLLaMA 3 BPE 450M\\n\\n10[20]\\nTotal Training FLOPs\\n\\n\\n0.72\\n\\n0.70\\n\\n\\nBPB vs Training Bytes at Fixed Inference FLOPs\\n\\nBLT Entropy ps=6 5.2B\\nBLT Entropy ps=8 6.4B\\nLLaMA 2 BPE 3.6B\\nLLaMA 3 BPE 3.9B\\n\\n10[21] 10[22]\\nTotal Training FLOPs\\n\\n|Col1|Col2|Col3|BLT Entropy ps=6 550M BLT Entropy ps=8 760M|\\n|---|---|---|---|\\n||||LLaMA 2 BPE 450M LLaMA 3 BPE 450M|\\n|||||\\n|||||\\n|||||\\n|0B bytes||50B bytes||\\n|5||1||\\n\\n|Col1|Col2|Col3|BLT Entrop|y ps=6 5.2B|\\n|---|---|---|---|---|\\n||||BLT Entrop LLaMA 2 B LLaMA 3 B|y ps=8 6.4B PE 3.6B PE 3.9B|\\n||||||\\n||||||\\n|||400B bytes|1T bytes||\\n\\n\\n**Figure 1 Scaling trends for fixed inference flop models (fully) trained with varying training budgets. In token-based**\\nmodels, a fixed inference budget determines the model size. In contrast, the BLT architecture provides a new scaling\\naxis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama 2 and 3. Moving to the larger\\ninference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and\\ncrossover point are indicated with vertical lines.\\n\\ncomplexity of predictions. Central to our architecture is the idea that models should dynamically allocate\\ncompute where it is needed. For example, a large transformer is not needed to predict the ending of most\\nwords, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new\\nsentence. This is reflected in BLT’s architecture (§3) where there are three transformer blocks: two small\\nbyte-level local models and a large global latent transformer (Figure 2). To determine how to group bytes into\\npatches and therefore how to dynamically allocate compute, BLT segments data based on the entropy of the\\nnext-byte prediction creating contextualized groupings of bytes with relatively uniform information density.\\n\\nWe present the first flop-controlled scaling study of byte-level models up to 8B parameters and 4T training\\nbytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization.\\nOverall, BLT matches training flop-controlled performance[1] of Llama 3 while using up to 50% fewer flops\\nat inference (§5). We also show that directly working with raw bytes provides significant improvements\\nin modeling the long-tail of the data. BLT models are more robust than tokenizer-based models to noisy\\ninputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge,\\nphonology, and low-resource machine translation tasks (§6). Finally, with BLT models, we can simultaneously\\nincrease model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on\\naverage, save compute which can be reallocated to grow the size of the global latent transformer, because it is\\nrun less often. We conduct inference-flop controlled scaling experiments (Figure 1), and observe significantly\\nbetter scaling trends than with tokenization-based architectures.\\n\\nIn summary, this paper makes the following contributions: 1) We introduce BLT, a byte latent llm architecture\\nthat dynamically allocates compute to improve flop efficiency, 2) We show that we achieve training flopcontrolled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics\\nfor flop efficiency gains of up to 50%, 3) BLT models unlock a new dimension for scaling llms, where model\\nsize can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness\\nof BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms\\n[miss. We release the training and inference code for BLT at https://github.com/facebookresearch/blt.](https://github.com/facebookresearch/blt)\\n\\n\\n### 2 Patching: From Individual Bytes to Groups of Bytes\\n\\nSegmenting bytes into patches allows BLT to dynamically allocate compute based on context. Figure 3 shows\\nseveral different methods for segmenting bytes into patches. Formally, a patching function fp segments a\\n\\n\\n1We calculate the computational cost of a model by counting the number of Floating Point OPerations (flops) needed.\\n\\n\\n-----\\n\\nB e t t e r _ t h a n _ B P E !\\n\\nLocal Decoder\\n\\n#### Latent Transformer\\n\\nLocal Encoder\\n\\n\\n5. Small Byte-Level Transf. Makes\\n**Next-Byte Prediction**\\n\\n4. Unpatching to Byte\\nSequence via Cross-Attn\\n\\n3. Large Latent Transformer\\n**Predicts Next Patch**\\n\\n2. Entropy-Based Grouping of\\nBytes Into Patches via Cross-Attn\\n\\n1. Byte-Level Small Transf. Encodes\\n**Byte Stream**\\n\\n|Latent Transformer|Col2|Col3|Col4|Col5|\\n|---|---|---|---|---|\\n||||||\\n||||||\\n\\n\\n**Figure 2 BLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations,**\\na computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode\\nthe next patch of bytes. BLT incorporates byte n-gram embeddings and a cross-attention mechanism to maximize\\ninformation flow between the Latent Transformer and the byte-level modules (Figure 5). Unlike fixed-vocabulary\\ntokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.\\n\\nsequence of bytes xxx = {xi, |i = 1, . . . n} of length n into a sequence of m < n patches ppp = {pj|j = 1, . . ., m}\\nby mapping each xi to the set {0,1} where 1 indicates the start of a new patch. For both token-based and\\npatch-based models, the computational cost of processing data is primarily determined by the number of\\nsteps executed by the main Transformer. In BLT, this is the number of patches needed to encode the data\\nwith a given patching function. Consequently, the average size of a patch, or simply patch size, is the main\\nfactor for determining the cost of processing data during both training and inference with a given patching\\nfunction (§4.5). Next, we introduce three patching functions: patching with a fixed number of bytes per\\npatch (§2.1), whitespace patching (§2.2), and dynamically patching with entropies from a small byte lm (§2.3).\\nFinally, we discuss incremental patching and how tokenization is different from patching (§2.4).\\n\\n#### 2.1 Strided Patching Every K Bytes\\n\\nPerhaps the most straightforward way to group bytes is into patches of fixed size k as done in MegaByte (Yu\\net al., 2023). The fixed stride is easy to implement for training and inference, provides a straightforward\\nmechanism for changing the average patch size, and therefore makes it easy to control the flop cost. However,\\nthis patching function comes with significant downsides. First, compute is not dynamically allocated to where\\nit is needed most: one could be either wasting a transformer step j if only predicting whitespace in code, or not\\nallocating sufficient compute for bytes dense with information such as math. Second, this leads to inconsistent\\nand non-contextual patching of similar byte sequences, such as the same word being split differently.\\n\\n\\n-----\\n\\n**Figure 3 Patching schemes group bytes in different ways, each leading to a different number of resulting patches. Since**\\neach patch is processed using a large transformer step, the number of patches directly determines the bulk of the\\ncompute expended in terms of flops. These schemes group bytes into patches by (a) striding every four bytes (§2.1)\\nas in MegaByte (Yu et al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3 (Dubey\\net al., 2024) tokenizer, (c & d) entropy-based patching as in this work (§2.3), (e) patching on space-bytes (Slagle, 2024),\\n(f) and patching on entropy using a small CNN byte-level model with 2-byte context.\\n\\n4\\n\\n3\\n\\n2\\n\\n1\\n\\n0\\n< D a e n e r y s _ T a r g a r y e n _ i s _ i n _ G a m e _ o f _ T h r o n e s, _ a _ f a n t a s y _ e p i c _ b y _ G e o r g e _ R . R . _ M a r t i n . >\\n\\n**Figure 4 This figure plots the entropy H(xi) of each byte in “Daenerys Targeryen is in Game of Thrones, a fantasy epic**\\nby George R.R. Martin.” with spaces shown as underscores. Patches end when H(xi) exceeds the global threshold θg,\\nshown as a red horizontal line. The start of new patches are shown with vertical gray lines. For example, the entropies\\nof “G” and “e” in “George R.R. Martin” exceed θg, so “G” is the start of a single byte patch and “e” of a larger patch\\nextending to the end of the named entity as the entropy H(xi) stays low, resulting in no additional patches.\\n\\n#### 2.2 Space Patching\\n\\nSlagle (2024) proposes a simple yet effective improvement over strided patching that creates new patches\\nafter any space-like bytes[2] which are natural boundaries for linguistic units in many languages. In Space\\npatching, a latent transformer step (i.e., more flops) is allocated to model every word. This ensures words\\nare patched in the same way across sequences and that flops are allocated for hard predictions which often\\nfollow spaces. For example, predicting the first byte of the answer to the question “Who composed the Magic\\nFlute? ” is much harder than predicting the remaining bytes after “M” since the first character significantly\\nreduces the number of likely choices, making the completion “Mozart” comparatively easy to predict. However,\\nspace patching cannot gracefully handle all languages and domains, and most importantly cannot vary the\\npatch size. Next, we introduce a new patching method that uses the insight that the first bytes in words are\\ntypically most difficult to predict, but that provides a natural mechanism for controlling patch size.\\n\\n#### 2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM\\n\\nRather than relying on a rule-based heuristic such as whitespace, we instead take a data-driven approach to\\nidentify high uncertainty next-byte predictions. We introduce entropy patching, which uses entropy estimates\\nto derive patch boundaries.\\n\\nWe train a small byte-level auto-regressive language model on the training data for BLT and compute next\\nbyte entropies under the LM distribution pe over the byte vocabulary V:\\n\\n_H(xi) =_ � _pe(xi = v|xxx<i) log pe(xi = v|xxx<i)_ (1)\\n\\n_v∈V_\\n\\nWe experiment with two methods to identify patch boundaries given entropies H(xi). The first, finds points\\nabove a global entropy threshold, as illustrated in Figure 4. The second, identifies points that are high\\n\\n2Space-like bytes are defined as any byte that is not a latin character, digit, or utf-8 continuation byte. In addition, each\\npatch must contain at least one non space-like byte.\\n\\n\\n-----\\n\\nrelative to the previous entropy. The second approach can also be interpreted as identifying points that break\\napproximate monotonically decreasing entropy withing the patch.\\n\\nGlobal Constraint _H(xt) > θg_\\nApprox. Monotonic Constraint _H(xt) −_ _H(xt−1) > θr_\\n\\nPatch boundaries are identified during a lightweight preprocessing step executed during dataloading. This is\\ndifferent from Nawrot et al. (2023) where classifier is trained to predict entropy-based patch boundaries. In\\nour experiments (§4), we compare these two methods for distinguishing between low and high entropy bytes.\\n\\n#### 2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching\\n\\nMany modern llms, including our baseline Llama 3, use a subword tokenizer like bpe (Gage, 1994; Sennrich\\net al., 2016). We use “tokens” to refer to byte-groups drawn from a finite vocabulary determined prior to\\ntraining as opposed to “patches” which refer to dynamically grouped sequences without a fixed vocabulary.\\nA critical difference between patches and tokens is that with tokens, the model has no direct access to the\\nunderlying byte features.\\n\\nA crucial improvement of BLT over tokenization-based models is that redefines the trade off between the\\nvocabulary size and compute. In standard llms, increasing the size of the vocabulary means larger tokens\\non average and therefore fewer steps for the model but also larger output dimension for the final projection\\nlayer of the model. This trade off effectively leaves little room for tokenization based approaches to achieve\\nsignificant variations in token size and inference cost. For example, Llama 3 increases the average token size\\nfrom 3.7 to 4.4 bytes at the cost of increasing the size of its embedding table 4x compared to Llama 2.\\n\\nWhen generating, BLT needs to decide whether the current step in the byte sequence is at a patch boundary\\nor not as this determines whether more compute is invoked via the Latent Transformer. This decision needs\\nto occur independently of the rest of the sequence which has yet to be generated. Thus patching cannot\\nassume access to future bytes in order to choose how to segment the byte sequence. Formally, a patching\\nscheme fp satisfies the property of incremental patching if it satisfies:\\n\\n_fp(xxx<i) = fp(xxx)<i_\\n\\nbpe is not an incremental patching scheme as the same prefix can be tokenized differently depending on the\\ncontinuation sequence, and therefore does not satisfy the property above[3].\\n\\n### 3 BLT Architecture\\n\\nBLT is composed of a large global autoregressive language model that operates on patch representations, along\\nwith two smaller local models that encode sequences of bytes into patches and decode patch representations\\nback into bytes (Figure 2).\\n\\n#### 3.1 Latent Global Transformer Model\\n\\n_The Latent Global Transformer is an autoregressive transformer model G with lG layers, which maps a sequence_\\nof latent input patch representations, pj into a sequence of output patch representations, oj. Throughout the\\npaper, we use the subscript j to denote patches and i to denote bytes. The global model uses a block-causal\\nattention mask (Dubey et al., 2024), which restricts attention to be up to and including the current patch\\nwithin the current document. This model consumes the bulk of the flops during pre-training as well as\\ninference, and thus, choosing when to invoke it allows us to control and vary the amount of compute expended\\nfor different portions of the input and output as a function of input/output complexity.\\n\\n3Using a special delimiter token to indicate patch boundaries can turn bpe into an incremental patching scheme but increases\\nthe byte-sequence length.\\n\\n\\n-----\\n\\n#### 3.2 Local Encoder\\n\\n_The Local Encoder Model, denoted by E, is a lightweight transformer-based model with lE << lG layers, whose_\\nmain role is to efficiently map a sequence of input bytes bi, into expressive patch representations, pj. A\\nprimary departure from the transformer architecture is the addition of a cross-attention layer after each\\ntransformer layer, whose function is to pool byte representations into patch representations (Figure 5). First,\\nthe input sequence of bytes, bi, are embedded using a R[256][×][h][E] matrix, denoted as xi. These embeddings are\\nthen optionally augmented with additional information in the form of hash-embeddings (§3.2.1). A series of\\nalternating transformer and cross-attention layers (§3.2.2) then transform these representations into patch\\nrepresentations, pi that are processed by the global transformer, G. The transformer layers use a local block\\n_causal attention mask; each byte attends to a fixed window of wE preceding bytes that in general can cross_\\nthe dynamic patch boundaries but can not cross document boundaries. The following subsections describe\\ndetails about the embeddings and the cross-attention block.\\n\\n**3.2.1** **Encoder Hash n-gram Embeddings**\\n\\nA key component in creating robust, expressive representations at each step i is to incorporate information\\nabout the preceding bytes. In BLT, we achieve this by modeling both the byte bi individually and as part of\\na byte n-gram. For each step i, we first construct byte-grams\\n\\n_gi,n = {bi−n+1, . . ., bi}_ (2)\\n\\nfor each byte position i and n from three to eight.[4]\\n\\nWe then introduce hash n-gram embeddings, that map all byte n-grams via a hash function to an index in an\\nembedding table En[hash] with a fixed size, for each size n ∈{3, 4, 5, 6, 7, 8} (Bai et al., 2010). The resulting\\nembedding is then added to the embedding of the byte before being normalized and passed as input to the\\nlocal encoder model. We calculate the augmented embedding\\n\\n_ei = xi +_ � _En[hash](Hash(gi,n))_ (3)\\n\\n_n=3,...,8_\\n\\nwhere, Hash(gi,n) = RollPolyHash(gi,n)%|En[hash]| (4)\\n\\nWe normalize ei by the number of n-grams sizes plus one and use RollPolyHash as defined in Appendix C. In\\nSection 7, we ablate the effects of n-gram hash embeddings with different values for n and embedding table\\nsize on flop-controlled scaling law trends. In addition to hash n-gram embeddings, we also experimented\\nwith frequency based n-gram embeddings, and we provide details of this exploration in Appendix D.\\n\\n**3.2.2** **Encoder Multi-Headed Cross-Attention**\\n\\nWe closely follow the input cross-attention module of the Perceiver architecture (Jaegle et al., 2021), with the\\nmain difference being that latent representations correspond to variable patch representations as opposed to a\\nfixed set of latent representations (Figure 5), and only attend to the bytes that make up the respective patch.\\nThe module comprises a query vector, corresponding to each patch pj, which is initialized by pooling the\\nbyte representations corresponding to patch pj, followed by a linear projection, EC ∈ R[h][E] _[×][(][h][E]_ _[×][U][E]_ [)], where UE\\nis the number of encoder cross-attention heads. Formally, if we let fbytes(pj) denote the sequence of bytes\\ncorresponding to patch, pj, then we calculate\\n\\n_P0,j = EC(fbytes((pj)), f is a pooling function_ (5)\\n\\n\\n_Pl = Pl−1 + Wo_\\n\\n\\n� � _QK_ _T_\\nsoftmax _√_\\n\\n_dk_\\n\\n\\n� �\\n_V_ (6)\\n\\n\\nwhere Qj = Wq(Pl−1,j), Ki = Wk(hl−1,i), Vi = Wv(hl−1,i) (7)\\n\\n_hl = Encoder-Transformer-Layerl(hl−1)_ (8)\\n\\nwhere P ∈ R[n][p][×][h][G] represents np patch representations to be processed by the global model, which is initialized\\nby pooling together the byte embeddings ei corresponding to each patch pj. Wq, Wk, Wv and Wo are the\\n\\n4We omit byte-grams of size n or more when i < n.\\n\\n\\n-----\\n\\n**Figure 5 The local encoder uses a cross-attention block with patch representations as queries, and byte representations**\\nas keys/values to encode byte representations into patch representations. The local decoder uses a similar block but\\nwith the roles reversed i.e. byte representations are now the queries and patch representations are the keys/values.\\nHere we use Cross-Attn k = 2.\\n\\nprojections corresponding to the queries, keys, values, and output where the keys and values are projections\\nof byte representations hi from the previous layer (ei for the first layer). We use a masking strategy specific\\nto patching where each query Qj only attends to the keys and values that correspond to the bytes in patch j.\\nBecause we use multi-headed attention over Q, K and V and patch representations are typically of larger\\ndimension (hG) than hE, we maintain Pl as multiple heads of dimension hE when doing cross-attention, and\\nlater, concat these representations into hG dimensions. Additionally, we use a pre-LayerNorm on the queries,\\nkeys and values and no positional embeddings are used in this cross-attention module. Finally, we use a\\nresidual connection around the cross-attention block.\\n\\n#### 3.3 Local Decoder\\n\\nSimilar to the local encoder, the local decoder D is a lightweight transformer-based model with lD << lG\\nlayers, that decodes a sequence of global patch representations oj, into raw bytes, yi. The local decoder\\npredicts a sequence of raw bytes, as a function of previously decoded bytes, and thus, takes as input the hidden\\nrepresentations produced by the local encoder for the byte-sequence. It applies a series of lD alternating\\nlayers of cross attention and transformer layers. The cross-attention layer in the decoder is applied before the\\ntransformer layer to first create byte representations from the patch representations, and the local decoder\\ntransformer layer operates on the resulting byte sequence.\\n\\n**3.3.1** **Decoder Multi-headed Cross-Attention**\\n\\nIn the decoder cross-attention, the roles of the queries and key/values are interchanged i.e. the byterepresentations are now the queries, and the patch representations are now the key/values. The initial\\nbyte-representations for the cross-attention are initialized as the byte embeddings from the last encoder layer\\ni.e. hlE . The subsequent byte-representations for layer l, dl,i are computed as:\\n\\n_D0 = hlE_ (9)\\n\\n\\n_Bl = Dl−1 + Wo_\\n\\n\\n� � _QK_ _T_\\nsoftmax _√_\\n\\n_dk_\\n\\n\\n� �\\n_V_ _,_ (10)\\n\\n\\nwhere Qi = Wq(dl−1,i), Ki = Wk(DC(oj)), Vi = Wv(DC(oj)) (11)\\n\\n_Dl = Decoder-Transformer-layerl(Bl)_ (12)\\n\\n\\n-----\\n\\nwhere once again, Wk, Wv are key/value projection matrices that operate on a linear transformation and split\\noperation DC, applied to the final patch representations oj from the global model, Wq is a query projection\\nmatrices operating on byte representations dl−1 from the previous decoder transformer layer (or hlE for the\\nfirst layer), and Wo is the output projection matrix, thus making B ∈ R[h][D][×][n][b], where nb is the number of\\noutput bytes. The next decoder representations Dl are computed using a decoder transformer layer on the\\noutput of the cross-attention block, B. As in the local encoder cross-attention, we use multiple heads in the\\nattention, use pre LayerNorms, no positional embeddings, and a residual connection around the cross-attention\\nmodule.\\n\\n### 4 Experimental Setup\\n\\nWe carefully design controlled experiments to compare BLT with tokenization based models with particular\\nattention to not give BLT any advantages from possibly using longer sequence contexts.\\n\\n#### 4.1 Pre-training Datasets\\n\\nAll model scales that we experiment in this paper are pre-trained on two datasets: 1) The Llama 2 dataset (Touvron et al., 2023), which comprises 2 trillion tokens collected from a variety of publicly available sources,\\nwhich are subsequently cleaned and filtered to improve quality; and 2) BLT-1T: A new dataset with 1 trillion\\ntokens gathered from various public sources, and also including a subset of the pre-training data released\\nby Datacomp-LM (Li et al., 2024). The former is used for scaling law experiments on optimal number of\\ntokens as determined by Dubey et al. (2024) to determine the best architectural choices for BLT, while the\\nlatter is used for a complete pre-training run to compare with Llama 3 on downstream tasks. Neither of these\\ndatasets include any data gathered from Meta products or services. Furthermore, for baseline experiments for\\ntokenizer-based models, we use the Llama 3 tokenizer with a vocabulary size of 128K tokens, which produced\\nstronger baseline performance that the Llama 2 tokenizer in our experiments.\\n\\n#### 4.2 Entropy Model\\n\\nThe entropy model in our experiments is a byte level language model trained on the same training distribution\\nas the full BLT model. Unless otherwise mentioned, we use a transformer with 100M parameters, 14 layers,\\nand a hidden dimensionality of 512, and sliding window attention of 512 bytes. The remaining hyperparameters\\nare the same as in our local and global transformers. We experimented with different model sizes, receptive\\nfields, and architectures as discussed in section 7. In particular, when the receptive field of the model is small\\nenough, the trained entropy model can be encoded in an efficient lookup table.\\n\\n#### 4.3 Entropy Threshold and Equalizing Context Length\\n\\nFor models using entropy-based patching, we estimate a patching threshold that achieves a desired average\\n_patch size on the pretraining data mix. In BLT, unlike with tokenization, the patch size can be arbitrarily_\\nchosen having significant implications on the context size used by the model. To maintain the same average\\ncontext length and avoid giving larger patch sizes unfair advantage, we ensure that the number of bytes in\\neach batch remains constant in expectation. This means that we reduce the sequence length of models with\\nlarger patch sizes. On Llama 2 data, we use a 8k byte context while on the BLT-1T dataset we increase the\\ncontext to 16k bytes on average while maintaining the same batch size of 16M bytes on average.\\n\\nWhile the average batch size is constant, when loading batches of data, dynamic patching methods yield\\ndifferent ratios of bytes to patches. For efficiency reasons, our implementation of BLT training packs batches\\nof patches to avoid padding steps in the more expensive latent transformer. This ensures that every batch has\\nthe same number of patches. During training we pad and possibly truncate byte sequences to 12k and 24k\\nbytes respectively for Llama 2 and BLT-1T datasets, to avoid memory spikes from sequences with unusually\\nlarge patches.\\n\\n\\n-----\\n\\n#### 4.4 Entropy Model Context\\n\\nEmpirically, we find that using entropy patching yields progressively larger patches in structured content like\\nmultiple choice tasks (see patching on an MMLU example in Figure 9) which are often very repetitive. These\\nvariations are caused by lower entropy on the repeated content found in the entropy model context. So for\\nthe large scale run of BLT-Entropy with patch size 4.5, we reset the entropy context with new lines and use\\napproximate monontonicity constraint as it suffers less from \"entropy drift\" from changes in context length.\\nThis change only affects how we compute entropies, but we still follow the same procedure to identify the\\nvalue of the entropy threshold.\\n\\n#### 4.5 FLOPs Estimation\\n\\nWe largely follow the equations for computation of transformer flops from Chinchilla (Hoffmann et al., 2022)\\ncomprising flops for the feed-forward layers, qkvo projections in the self-attention layer, and computation\\nof attention and output projection. A notable difference is that we assume the input embedding layer is\\nimplemented as an efficient lookup instead of a dense matrix multiplication, therefore becoming a 0-flop\\noperation. Following previous work, we estimate that the backwards pass has twice the number of flops as\\nthe forward pass.\\n\\nTo compute flops per byte for BLT models, we add up the flops for the local encoder transformer, the\\nglobal latent transformer, and the local decoder transformer, together with the cross attention blocks in the\\nencoder and the decoder:\\n\\nFLBLT = Transf. FL(hG, lG, m = nctx/np, V = 0)/np (13)\\n\\n+ Transf. FL(hE _, lE_ _, m = wE_ _, V = 0)_ (14)\\n\\n+ Transf. FL(hD, lD, m = wD, V = 256) (15)\\n\\n+ Cross Attn. FL(hE _, lE_ _, m = np, r = np/k) × k/np_ (16)\\n\\n+ Cross Attn. FL(hD, lD, m = k, r = k/np) (17)\\n\\nwhere nctx is the sequence length in bytes, np is the patch size, r is the ratio of queries to key/values, k is\\nthe ratio of patch-dimension to byte-dimension i.e. the number of local model splits that concatenate to\\nform a global model representation (k = 2 in Figure 5). V corresponds to the vocabulary size for the output\\nprojection, which is only used in the local decoder. Depending on whether a module is applied on the byte or\\npatch sequence, the attention uses a different context length, m. We modify the attention flops accordingly\\nfor each component. The exact equations for flops computation for Transformer-FLOPs and Cross-Attention\\nFLOPs are provided in Appendix B.\\n\\n#### 4.6 Bits-Per-Byte Estimation\\n\\nPerplexity only makes sense in the context of a fixed tokenizer as it is a measure of the uncertainty for each\\ntoken. When comparing byte and token-level models, following previous work (Xue et al., 2022; Yu et al.,\\n2023; Wang et al., 2024), we instead report Bits-Per-Byte (BPB), a tokenizer independent version of perplexity.\\nSpecifically:\\n\\nBPB(x) = _LCE(xxx)_ (18)\\nln(2) · nbytes\\n\\nwhere the uncertainty over the data xxx as measured by the sum of the cross-entropy loss is normalized by the\\ntotal number of bytes in xxx and a constant.\\n\\n#### 4.7 Transformer Architecture Hyperparameters\\n\\nFor all the transformer blocks in BLT, i.e. both local and global models, we largely follow the architecture of\\nLlama 3 (Dubey et al., 2024); we use the SwiGLU activation function (Shazeer, 2020) in the feed-forward\\nlayers, rotary positional embeddings (RoPE) (Su et al., 2021) with θ = 500000 (Xiong et al., 2024) only\\n\\n\\n-----\\n\\nBPB vs Training FLOPs at Compute Optimal Ratio (Entropy Patching)\\n\\nBLT Entropy ps=4\\nBLT Entropy ps=8\\nLLaMA 2 BPE\\nLLaMA 3 BPE\\nMegabyte++ ps=4\\nMegabyte++ ps=6\\n\\n10[21] 10[22]\\nTotal Training FLOPs\\n\\n\\n0.86\\n\\n0.84\\n\\n0.82\\n\\n0.80\\n\\n0.78\\n\\n0.76\\n\\n0.74\\n\\n0.72\\n\\n\\nBPB vs Training FLOPs at Compute Optimal Ratio (Space Patching)\\n\\nBLT Space ps=6\\nBLT Space w/o cross-attn\\nLLaMA 3 BPE\\nMegabyte++ ps=4\\nMegabyte++ ps=6\\nSpaceByte\\n\\n10[21] 10[22]\\nTotal Training FLOPS\\n\\n\\n0.86\\n\\n0.84\\n\\n0.82\\n\\n0.80\\n\\n0.78\\n\\n0.76\\n\\n0.74\\n\\n0.72\\n\\n|Col1|Col2|Col3|BLT Space ps=6|Col5|Col6|\\n|---|---|---|---|---|---|\\n||||BLT Space w/o cross-at LLaMA 3 BPE||tn|\\n||||Megabyte++ ps=4 Megabyte++ ps=6|||\\n|||SpaceByte||||\\n|||||||\\n|||||||\\n|||||||\\n|||||||\\n|||||||\\n\\n|Col1|Col2|Col3|BLT Entropy ps|=4|\\n|---|---|---|---|---|\\n||||BLT Entropy ps LLaMA 2 BPE|=8|\\n||||LLaMA 3 BPE Megabyte++ p|s=4|\\n|||Megabyte++ p||s=6|\\n||||||\\n||||||\\n||||||\\n||||||\\n||||||\\n\\n\\n**Figure 6 Scaling trends for BLT models with different architectural choices, as well as for baseline BPE token-based**\\nmodels. We train models at multiple scales from 1B up to 8B parameters for the optimal number of tokens as computed\\nby Dubey et al. (2024) and report bits-per-byte on a sample from the training distribution. BLT models perform\\non par with state-of-the-art tokenizer-based models such as Llama 3, at scale. PS denotes patch size. We illustrate\\nseparate architecture improvements on space-patching (left) and combine them with dynamic patching (right).\\n\\nin self-attention layers, and RMSNorm (Zhang and Sennrich, 2019) for layer normalization. We use Flash\\nattention (Dao et al., 2022) for all self-attention layers that use fixed-standard attention masks such as block\\n_causal or fixed-window block causal, and a window size of 512 for fixed-width attention masks. Since our_\\ncross-attention layers involve dynamic patch-dependent masks, we use Flex Attention[5] to produce fused\\nimplementations and significantly speed up training.\\n\\n\\n#### 4.8 BLT-Specific Hyperparameters\\n\\nTo study the effectiveness of BLT models, we conduct experiments along two directions, scaling trends, and\\ndownstream task evaluations, and we consider models at different scales: 400M, 1B, 2B, 4B and 8B for these\\nexperiments. The architecture hyperparameters for these models are presented in Appendix Table 10. We use\\nmax-pooling to initialize the queries for the first cross-attention layer in the local encoder. We use 500, 000\\nhashes with a single hash function, with n-gram sizes ranging from 3 to 8, for all BLT models. We use a\\nlearning rate of 4e − 4 for all models. The choice of matching learning rate between token and BLT models\\nfollows a hyperparameter search between 1e − 3 and 1e − 4 at 400M and 1B model scales showing the same\\nlearning rate is optimal. For scaling trends on Llama-2 data, we use training batch-sizes as recommended\\nby Dubey et al. (2024) or its equivalent in bytes. For optimization, we use the AdamW optimizer (Loshchilov\\nand Hutter, 2017) with β1 set to 0.9 and β2 to 0.95, with an ϵ = 10[−][8]. We use a linear warm-up of 2000 steps\\nwith an cosine decay schedule of the learning rate to 0, we apply a weight decay of 0.1, and global gradient\\nclipping at a threshold of 1.0.\\n\\n\\n### 5 Scaling Trends\\n\\nWe present a holistic picture of the scaling trends of byte-level models that can inform further scaling of BLT\\nmodels. Our scaling study aims to address the limitations of previous research on byte-level models in the\\nfollowing ways: (a) We compare trends for the compute-optimal training regime, (b) We train matching 8B\\nmodels on non-trivial amounts of training data (up to 1T tokens/4T bytes) and evaluate on downstream tasks,\\nand (c) We measure scaling trends in inference-cost controlled settings. In a later section, we will investigate\\nspecific advantages from modeling byte-sequences.\\n\\n\\n[5https://pytorch.org/blog/flexattention](https://pytorch.org/blog/flexattention)\\n\\n\\n-----\\n\\n#### 5.1 Parameter Matched Compute Optimal Scaling Trends\\n\\nUsing the Llama 2 dataset, we train various compute-optimal bpe and BLT models across four different sizes,\\nranging from 1B to 8B parameters. We then plot the training flops against language modeling performance\\non a representative subset of the training data mixture. The bpe models are trained using the optimal ratio\\nof model parameters to training data, as determined by Llama 3 (Dubey et al., 2024). This compute-optimal\\nsetup is theoretically designed to achieve the best performance on the training dataset within a given training\\nbudget (Hoffmann et al., 2022), providing a robust baseline for our model. For each bpe model, we also\\ntrain a corresponding BLT model on the same data, using a Latent Transformer that matches the size and\\narchitecture of the corresponding bpe Transformer.\\n\\nAs illustrated in Figure 6 (right), BLT models either match or outperform their bpe counterparts and this\\ntrend holds as we scale model size and flops. To the best of our knowledge, BLT is the first byte-level\\nTransformer architecture to achieve matching scaling trends with BPE-based models at compute optimal\\nregimes. This therefore validates our assumption that the optimal ratio of parameters to training compute for\\nbpe also applies to BLT, or at least it is not too far off.\\n\\nBoth architectural improvements and dynamic patching are crucial to match bpe scaling trends. In Figure 6\\n(left), we compare space-patching-based models against Llama 3. We approximate SpaceByte (Slagle, 2024)\\nusing BLT space-patching without n-gram embeddings and cross-attention. Although SpaceByte improves\\nover Megabyte, it remains far from Llama 3. In Figure 6 (right), we illustrate the improvements from both\\narchitectural changes and dynamic patching. BLT models perform on par with state-of-the-art tokenizer-based\\nmodels such as Llama 3, at scale.\\n\\nWe also observe the effects of the choice of tokenizer on performance for tokenizer-based models, i.e., models\\ntrained with the Llama-3 tokenizer outperform those trained using the Llama-2 tokenizer on the same training\\ndata.\\n\\nFinally, our BLT architecture trends between Llama 2 and 3 when using significantly larger patch sizes. The\\nbpe tokenizers of Llama 2 and 3 have an average token size of 3.7 and 4.4 bytes. In contrast, BLT can\\nachieve similar scaling trends with an average patch size of 6 and even 8 bytes. Inference flop are inversely\\nproportional to the average patch size, so using a patch size of 8 bytes would lead to nearly 50% inference\\nflop savings. Models with larger patch sizes also seem to perform better as we scale model and data size.\\nBLT with patch size of 8 starts at a significantly worse point compared to bpe Llama 2 at 1B but ends up\\nbetter than bpe at 7B scale. This suggests that such patch sizes might perform better at even larger scales\\nand possibly that even larger ones could be feasible as model size and training compute grow.\\n\\n#### 5.2 Beyond Compute Optimal Task Evaluations\\n\\nTo assess scaling properties further, we train an 8B BLT model beyond the compute optimal ratio on the\\nBLT-1T dataset, a larger higher-quality dataset, and measure performance on a suite of standard classification\\nand generation benchmarks. For evaluation, we select the following common sense reasoning, world knowledge,\\nand code generation tasks:\\n\\n_Classification tasks_ include ARC-Easy (0-shot) (Clark et al., 2018), Arc-Challenge (0-shot) (Clark et al., 2018),\\nHellaSwag (0-shot) (Zellers et al., 2019), PIQA (0-shot) (Bisk et al., 2020), and MMLU (5-shot) (Hendrycks\\net al., 2020). We employ a prompt-scoring method, calculating the likelihood over choice characters, and\\nreport the average accuracy.\\n\\n_Coding related generation tasks:_ We report pass@1 scores on MBPP (3-shot) (Austin et al., 2021) and\\nHumanEval (0-shot) (Chen et al., 2021), to evaluate the ability of LLMs to generate Python code.\\n\\nIn Table 1, we compare three models trained on the BLT-1T dataset: a bpe Llama 3 tokenizer-based model,[6]\\n\\nand two variants of the BLT model. One employing a space-patching scheme (BLT-Space) and another\\nutilizing an entropy-based patching scheme (BLT-Entropy). with approx. monotonicity constraint and reset\\nthe context of the entropy model with new lines (as discussed in subsection 4.4). All three models are\\n\\n6We choose the Llama 3 tokenizer with its 128k vocabulary as it performs better than Llama 2’s 32k vocabulary.\\n\\n\\n-----\\n\\nLlama 3 BLT-Space BLT-Entropy\\n1T Tokens 6T Bytes 4.5T Bytes\\n\\n**Arc-E** 77.6 75.4 **79.6**\\n**Arc-C** **53.3** 49.8 52.1\\n**HellaSwag** 79.1 79.6 **80.6**\\n**PIQA** 80.7 **81.1** 80.6\\n**MMLU** **58.1** 54.8 57.4\\n**MBPP** 40.2 37.6 **41.8**\\n**HumanEval** 31.1 27.4 **35.4**\\n\\n**Average** 60.0 58.0 **61.1**\\n\\n**Bytes/Patch on Train Mix** 4.4 **6.1** 4.5\\n\\n**Table 1 Comparison of flop-matched BLT 8B models trained on the BLT-1T dataset comprising high-quality tokens**\\nof text and code from publicly available sources, with baseline models using the Llama 3 tokenizer. BLT performs\\nbetter than Llama 3 on average, and depending on the patching scheme, achieves significant flops savings with a\\nminor reduction in performance.\\n\\nLlama 2 Llama 3 Entropy ps=6 Entropy ps=8 Inference flops Compute Optimal (Bytes) Crossover (Bytes)\\n\\n470m 450m 610m (1.2x) 760m (1.6x) 3.1E8 50B 150B\\n3.6B 3.9B 5.2B (1.3x) 6.6B (1.7x) 2.1E9 400B 1T\\n\\n**Table 2 Details of models used in the fixed-inference scaling study. We report non-embedding parameters for each**\\nmodel and their relative number compared to Llama 2. We pick model sizes with equal inference flops per byte. We\\nalso indicate BPE’s compute-optimal training data quantity and the crossover point where BLT surpasses BPE as seen\\nin Figure 1 (both expressed in bytes of training data). This point is achieved at much smaller scales compared to\\nmany modern training budgets.\\n\\ntrained with an equivalent flop budget. However, with BLT-Entropy we additionally make an inference time\\nadjustment of the entropy threshold from 0.6 to 0.1 which we find to improve task performance at the cost of\\nmore inference steps.\\n\\nThe BLT-Entropy model outperforms the Llama 3 model on 4 out of 7 tasks while being trained on the same\\nnumber of bytes. This improvement is like due to a combination of (1) a better use of training compute via\\ndynamic patching, and (2) the direct modeling of byte-level information as opposed to tokens.\\n\\nOn the other hand, BLT-Space underperforms the Llama 3 tokenizer on all but one task, but it achieves a\\nsignificant reduction in inference flops with its larger average patch size of 6 bytes. In comparison, the bpe\\nand entropy-patching based models have roughly equivalent average patch size of approximately 4.5 bytes on\\nthe training data mix. With the same training budget, the larger patch size model covers 30% more data\\nthan the other two models which might push BLT further away from the compute-optimal point.\\n\\n#### 5.3 Patches Scale Better Than Tokens\\n\\nWith BLT models, we can simultaneously increase model size and patch size while maintaining the same\\ntraining and inference flop budget and keeping the amount of training data constant. Arbitrarily increasing\\nthe patch size is a unique feature of patch-based models which break free of the efficiency tradeoffs of\\nfixed-vocabulary token-based models, as discussed in Section 2.4. Longer patch sizes save compute, which can\\nbe reallocated to grow the size of the global latent transformer, because it is run less often.\\n\\nWe conduct a fixed inference scaling study to test the hypothesis that larger models taking fewer steps on\\nlarger patches might perform better than smaller models taking more steps. Starting from model sizes of 400m\\nand 3.6B parameters with the Llama 2 tokenizer, we find flop equivalent models with the Llama 3 tokenizer\\nand BLT-Entropy models with average patch sizes of 6 and 8 bytes on the training datamix (see Table 2 for\\nmodel details). For patch size 8 models, we use 3 encoder layers instead of 1. We train each model for various\\ntraining flop budgets.\\n\\n\\n-----\\n\\nLlama 3 Llama 3.1 BLT\\n(1T tokens) (16T tokens) (1T tokens)\\n\\n**HellaSwag Original** 79.1 80.7 **80.6**\\n**HellaSwag Noise Avg.** 56.9 64.3 **64.3**\\n\\n**- AntSpeak** 45.6 61.3 **57.9**\\n\\n**- Drop** 53.8 57.3 **58.2**\\n\\n**- RandomCase** 55.3 65.0 **65.7**\\n\\n**- Repeat** 57.0 61.5 **66.6**\\n\\n**- UpperCase** 72.9 76.5 **77.3**\\n\\n**Phonology-G2P** 11.8 18.9 **13.0**\\n\\n**CUTE** 27.5 20.0 **54.1**\\n\\n**- Contains Char** 0.0 0.0 **55.9**\\n\\n**- Contains Word** 55.1 21.6 **73.5**\\n\\n**- Del Char** 34.6 34.3 **35.9**\\n\\n**- Del Word** **75.5** 84.5 56.1\\n\\n**- Ins Char** 7.5 0.0 **7.6**\\n\\n**- Ins Word** **33.5** 63.3 31.2\\n\\n**- Orthography** 43.1 0.0 **52.4**\\n\\n**- Semantic** 65 0.0 **90.5**\\n\\n**- Spelling** 1.1              - **99.9**\\n\\n**- Spelling Inverse** 30.1 3.6 **99.9**\\n\\n**- Substitute Char** 0.4 1.2 **48.7**\\n\\n**- Substitute Word** 16.4 6.8 **72.8**\\n\\n**- Swap Char** 2.6 2.4 **11.5**\\n\\n**- Swap Word** 20.1 4.1 **21**\\n\\n**Table 3 We compare our 8B BLT model to 8B BPE Llama 3 trained on 1T tokens on tasks that assess robustness to**\\nnoise and awareness of the constituents of language (best result bold). We also report the performance of Llama 3.1 on\\nthe same tasks and underline best result overall. BLT outperforms the Llama 3 BPE model by a large margin and\\neven improves over Llama 3.1 in many tasks indicating that the byte-level awareness is not something that can easily\\nbe obtained with more data.\\n\\nFigure 1 shows that BLT models achieve better scaling trends than tokenization-based architectures for both\\ninference flop classes. In both cases, BPE models perform better with small training budgets and are quickly\\nsurpassed by BLT not far beyond the compute-optimal regime. In practice, it can be preferable to spend\\nmore during the one-time pretraining to achieve a better performing model with a fixed inference budget. A\\nperfect example of this is the class of 8B models, like Llama 3.1, which has been trained on two orders of\\nmagnitude more data than what is compute-optimal for that model size.\\n\\nThe crossover point where BLT slightly improves over token-based models has shifted closer to the computeoptimal point when moving to the larger flop class models (from 3x down to 2.5x the compute optimal\\nbudget). Similarly, the larger patch size 8 model has steeper scaling trend in the larger flop class. As\\ndiscussed in Section 5.1, larger patch sizes appear to perform closer to BPE models at larger model scales.\\nWe attribute this, in part, to the decreasing share of total flops used by the byte-level Encoder and Decoder\\nmodules which seem to scale slower than the Latent Transformer. When growing total parameters 20x from\\n400M to 8B, we only roughly double BLT’s local model parameters. This is important as larger patch sizes\\nonly affect flops from the patch Latent Transformer and not the byte-level modules. In fact, that is why the\\nBLT-Entropy ps=8 went from 1.6x to 1.7x of the Llama 2 model size when moving to the larger model scale.\\n\\nIn summary, our patch-length scaling study demonstrates that the BLT patch-based architecture can achieve\\nbetter scaling trends by simultaneously increasing both patch and model size. Such trends seem to persist\\nand even improve at larger model scales.\\n\\n\\n-----\\n\\nLanguage Language → English English → Language\\n\\nLlama 3 BLT Llama 3 BLT\\n\\n**Arabic** 22.3 24.6 10.4 8.8\\n**German** 41.3 42.0 29.8 31.2\\n**Hindi** 20.7 20.9 7.8 7.2\\n**Italian** 34.0 33.9 24.4 26.2\\n**Vietnamese** 31.2 31.0 28.4 23.7\\n**Thai** 17.9 18.1 10.5 7.7\\n\\n**Armenian** 1.7 6.3 0.6 0.9\\n**Amharic** 1.3 3.1 0.4 0.5\\n**Assamese** 2.7 5.4 0.8 1.6\\n**Bengali** 4.7 12.7 1.7 4.1\\n**Bosnian** 36.0 37.3 16.9 19.6\\n**Cebuano** 18.2 20.6 5.8 9.1\\n**Georgian** 1.7 7.4 1.0 2.5\\n**Gujarati** 2.0 5.8 1.0 2.2\\n**Hausa** 5.75 5.9 1.2 1.3\\n**Icelandic** 16.1 17.9 4.8 5.3\\n**Kannada** 1.6 3.9 0.7 1.7\\n**Kazakh** 5.6 7.0 1.0 2.6\\n**Kabuverdianu** 20.3 20.9 5.1 6.8\\n**Khmer** 4.4 9.5 0.8 0.8\\n**Kyrgyz** 4.6 5.1 0.9 2.0\\n**Malayalam** 1.8 3.5 0.7 1.4\\n**Odia** 1.6 2.7 0.8 1.1\\n**Somali** 5.0 5.0 1.1 1.4\\n**Swahili** 10.1 12.0 1.4 2.3\\n**Urdu** 9.3 9.5 2.0 1.4\\n**Zulu** 4.7 5.0 0.6 0.5\\n\\n**Overall Average** 12.1 **14.0** 5.9 **6.4**\\n\\n**Table 4 Performance of 8B BLT and 8B Llama 3 trained for 1T tokens on translating into and from six widely-used**\\nlanguages and twenty one lower resource languages with various scripts from the FLORES-101 benchmark (Goyal\\net al., 2022).\\n\\n### 6 Byte Modeling Improves Robustness\\n\\nWe also measure the robustness of BLT compared to token-based models that lack direct byte-level information,\\nand present an approach to byte-ify pretrained token-based models.\\n\\n#### 6.1 Character-Level Tasks\\n\\nA very early motivation for training byte-level models was to take advantage of their robustness to byte\\nlevel noise in the input, and also to exploit their awareness of the constituents of tokens, which current\\ntokenizer-based models struggle with. To measure these phenomena, we perform additional evaluations on\\nbenchmarks that evaluate both robustness to input noise as well as awareness of characters, both English and\\nmulti-lingual, including digits and phonemes. We present these results in Table 3.\\n\\n_Noisy Data_ We create noised versions of the benchmark classification tasks described in Section 5.2, to\\ncompare the robustness of tokenizer-based models with that of BLT. We employ five distinct character-level\\nnoising strategies to introduce variations in the text: (a) AntSpeak : This strategy converts the entire text into\\nuppercase, space-separated characters. (b) Drop: Randomly removes 10% of the characters from the text. (c)\\n\\n\\n-----\\n\\n|Task|Prompt|Llama 3|BLT|\\n|---|---|---|---|\\n|Substitute Word|Question: Substitute \" and \" with \" internet \" in \" She went to the kitchen and saw two cereals. \". Answer:|She went to the kitchen and saw two cereals.|She went to the kitchen internet saw two cereals.|\\n|Swap Char|Question: Swap \" h \" and \" a \" in \" that \". Answer:|that|taht|\\n|Substitute Char|Question: Substitute \" a \" with \" m \" in \" page \". Answer:|-|pmge|\\n|Semantic Similarity|Question: More semantically related to \" are \": \" seem \", \" acre \". Answer:|acre|seem|\\n|Orthographic Similarity|Question: Closer in Levenshtein distance to \" time \": \" timber \", \" period \". Answer:|period|timber|\\n|Insert Char|Question: Add an \" z \" after every \" n \" in \" not \". Answer:|znotz|nzot|\\n\\n\\n**Figure 7 Output responses from Llama 3 and BLT models for various tasks from CUTE benchmark. BLT model**\\nperforms better on sequence manipulation tasks compared to the tokenizer-based Llama 3 model. Note that few-shot\\nexamples are not shown in the above prompts to maintain clarity.\\n\\n_RandomCase: Converts 50% of the characters to uppercase and 50% to lowercase randomly throughout the_\\ntext. (d) Repeat: Repeats 20% of the characters up to a maximum of four times. (e) UpperCase: Transforms\\nall characters in the text to uppercase. During evaluation, we apply each noising strategy to either the prompt,\\ncompletion, or both as separate tasks and report the average scores. In Table 3 we report results on noised\\nHellaSwag (Zellers et al., 2019) and find that BLT indeed outperforms tokenizer-based models across the\\nboard in terms of robustness, with an average advantage of 8 points over the model trained on the same data,\\nand even improves over the Llama 3.1 model trained on a much larger dataset.\\n\\n_Phonology - Grapheme-to-Phoneme (G2P)_ We assess BLT’s capability to map a sequence of graphemes\\n(characters representing a word) into a transcription of that word’s pronunciation (phonemes). In Table 3, we\\npresent the results of the G2P task in a 5-shot setting using Phonology Bench (Suvarna et al., 2024) and find\\nthat BLT outperforms the baseline Llama 3 1T tokenizer-based model on this task.\\n\\n_CUTE_ To assess character-level understanding, we evaluate BLT on the CUTE benchmark (Edman et al.,\\n2024), which comprises several tasks that are broadly classified into three categories: understanding composition,\\nunderstanding orthographic similarity, and ability to manipulate sequences. This benchmark poses a significant\\nchallenge for most tokenizer-based models, as they appear to possess knowledge of their tokens’ spellings\\nbut struggle to effectively utilize this information to manipulate text. Table 3 shows that BLT-Entropy\\noutperforms both BPE Llama 3 models by more than 25 points on this benchmark. In particular, our model\\ndemonstrates exceptional proficiency in character manipulation tasks achieving 99.9% on both spelling tasks.\\nSuch large improvements despite BLT having been trained on 16x less data than Llama 3.1 indicates that\\ncharacter level information is hard to learn for BPE models. Figure 7 illustrates a few such scenarios where\\nLlama 3 tokenizer model struggles but our BLT model performs well. Word deletion and insertion are the\\nonly two tasks where BPE performs better. Such word manipulation might not be straightforward for a\\nbyte-level model but the gap is not too wide and building from characters to words could be easier than the\\nother way around. We use the same evaluation setup in all tasks and the original prompts from Huggingface.\\nBPE models might benefit from additional prompt engineering.\\n\\n_Low Resource Machine Translation_ We evaluate BLT on translating into and out of six popular language\\nfamilies and twenty one lower resource languages with various scripts from the FLORES-101 benchmark (Goyal\\net al., 2022) and report SentencePiece BLEU in Table 4. Our results demonstrate that BLT outperforms a\\nmodel trained with the Llama 3 tokenizer, achieving a 2-point overall advantage in translating into English\\nand a 0.5-point advantage in translating from English. In popular language pairs, BLT performs comparably\\nto or slightly better than Llama 3. However, BLT outperforms Llama 3 on numerous language pairs within\\n\\n\\n-----\\n\\nLlama 3\\n8B\\n(220B tokens)\\n\\n\\nBLT\\n8B\\n(220B tokens)\\n\\n\\nBLT from Llama 3.1\\n8B\\n(220B tokens)\\n\\n\\nLlama 3.1\\n8B\\n(15T tokens)\\n\\n\\n**Arc-E** 67.4 66.8 66.6 83.4\\n**Arc-C** 40.4 38.8 45.8 55.2\\n**HellaSwag** 71.2 72.2 76.1 80.7\\n**PIQA** 77.0 78.2 77.4 80.7\\n**MMLU** 26.5 25.2 63.7 66.3\\n**MBPP** 11.8 10.0 38.2 47.2\\n**HumanEval** 9.2 7.3 34.2 37.2\\n\\n**Table 5 Initializing the global transformer model of BLT from the non-embedding parameters of Llama 3 improves**\\nperformance on several benchmark tasks. First three models trained on the Llama 2 data for compute-optimal steps.\\n\\nlower-resource language families, underscoring the effectiveness of byte modeling for generalizing to long-tail\\nbyte sequences.\\n\\n#### 6.2 Training BLT from Llama 3\\n\\nWe explore a workflow where BLT models can leverage existing pre-trained tokenizer-based models for better\\nand faster training convergence, acheived by initializing the global transformer parameters of BLT with\\nthose of a pre-trained Llama 3.1 model. Subsequently, we update the weights of the global transformer using\\none-tenth the learning rate employed for the local encoder and local decoder model, for Llama 3 optimal\\nnumber of steps, and present a comparison with a baseline BLT in Table 5. It is evident that BLT from\\nLlama 3.1 significantly outperforms both the Llama 3 and BLT baselines, which were trained with the same\\nnumber of flops. Moreover, when compared to our BLT-Entropy model (as presented in Table 1), which was\\ntrained on a significantly larger dataset (1T tokens), BLT from Llama 3.1 still achieves superior performance\\non MMLU task, suggesting that it can be an effective approach in significantly reducing the training flops.\\n\\nThis setup can also be viewed as transforming tokenizer-based models into tokenizer-free ones, effectively\\nconverting a pre-trained LLaMA 3.1 model into a BLT model. To provide a comprehensive comparison, we\\ninclude the original LLaMA 3.1 model trained on 15T tokens in Table 5 and evaluate it against the BLT\\nderived from LLaMA 3. Our model experiences a slight performance decline on MMLU and HumanEval,\\nbut a more significant drop on other tasks. This suggests that further work is needed to fully leverage the\\npre-trained model and improve upon its performance, particularly in terms of optimizing data mixtures and\\nother hyperparameters.\\n\\n### 7 Ablations and Discussion\\n\\nIn this section, we discuss ablations justifying architectural choices for BLT and the patching scheme and\\nhyper-parameters for the BLT 8B parameter model trained on the BLT-1T dataset.\\n\\n_Entropy Model Hyper-parameters_ To study the effect of varying entropy model size and context window\\nlength on scaling performance, we train byte-level entropy transformer models of different model sizes between\\n1m and 100m parameters, with varying context window lengths from 64 to 512. We plot bpb vs training flop\\nscaling law curves, created using our 400m and 1b BLT models trained on the Llama-2 dataset and present\\nthem in Figure 8. We find that scaling performance is positively correlated with both these dimensions of the\\nentropy model, with diminishing returns when we scale beyond 50m parameters.\\n\\n_Types of Patching_ We ablate the four different patching schemes, introduced in Section 2 i.e. 1) Strided\\nPatching with a stride of 4 and 6, 2) Patching on whitepsace, 3) BPE Tokenizer patching based on the Llama\\n3 tokenizer, and 4) Entropy based patching using a small byte llm.\\n\\n\\n-----\\n\\n1.05\\n\\n1.00\\n\\n\\n0.95\\n\\n0.90\\n\\n\\n0.85\\n\\n\\nBPB vs Training FLOPs at Compute Optimal Ratio\\n\\nP=100m,w=512\\nP= 10m,w=128\\nP= 10m,w=512\\nP=  1m,w=512\\nP= 50m,w=512\\nP=  1m,w= 64\\n\\n10[20] 2 × 10[20] 3 × 10[20]\\n\\nTotal Training FLOPS\\n\\n|Col1|Col2|P=100m,w=512 P= 10m,w=128 P= 10m,w=512|\\n|---|---|---|\\n||P= 1m,w=512 P= 50m,w=512 P= 1m,w= 64||\\n||||\\n||||\\n||||\\n\\n\\n**Figure 8 Variation of language modeling performance in bits-per-byte (bpb) with training flops for 400m and 1b**\\nBLT models patched with entropy models of different sizes and context windows. Both dimensions improve scaling\\nperformance, with diminishing returns beyond 50m parameter entropy models with a context of 512 bytes.\\n\\nLlama 3 Space Patching Entropy\\nBPE BLT BLT\\n\\n\\n**Arc-E** 67.4 67.2 68.9\\n**Arc-C** 40.5 37.6 38.3\\n**HellaSwag** 71.3 70.8 72.7\\n**PIQA** 77.0 76.5 77.6\\n\\n**Table 6 Benchmark evaluations of two patching schemes for 8b BLT models and BPE Llama3 baseline. These models**\\nare trained on the Llama 2 data for the optimal number of steps as determined by Dubey et al. (2024).\\n\\n\\nWhile dynamic patching reduces the effective length of sequences, we control for the sequence length to\\nmaintain a similar context length for all patching schemes. All the models see the same number of bytes in\\neach sequence during training and inference in expectation to prevent any confounding factors from being\\nable to model larger contexts. Figure 6 highlights the results of these ablations. All the remaining patching\\nschemes outperform static patching, with space patching being a very close competitor to dynamic entropy\\nbased patching.\\n\\nIn Table 6, we present benchmark evaluations for BLT models comparing tokenizer-based models, space\\npatching, and entropy-based patching, trained on the Llama 2 dataset for an optimal number of steps (Dubey\\net al., 2024). Although space patching is a simpler strategy that does not involve running an entropy model\\non the fly during training, we find that the gains we observed using entropy-based patching on scaling\\ntrends (Section 5) do indeed carry forward even to downstream benchmark tasks.[7]\\n\\n_Cross-Attention_ In Table 7, we ablate including cross-attention at various points in the encoder and decoder\\nof BLT. For the encoder cross-attention we test initializing the queries with 1) the same learned embedding\\nfor every global state, 2) a hash embedding of the bytes in the patch, and 3) pooling of the encoder hidden\\nrepresentation of the patch bytes at the given encoder layer.\\n\\nWe find that using cross-attention in the decoder is most effective. In the encoder, there is a slight improvement\\nin using cross-attention but only with pooling initialization of queries. Additionally, we find that cross-attention\\nhelps particularly on Common-Crawl and especially with larger patch sizes.\\n\\n\\n7Space patching results are from earlier runs without cross-attention, but similar trends are observed even with cross-attention.\\n\\n\\n-----\\n\\nBPB\\n\\nCross Attn. Dec. Cross Attn. Enc. Pooling Init Wikipedia CC Github Train Dist\\n\\n          - All Layers False 0.830 0.915 **0.442** 0.891\\n          - Last Layer False 0.836 0.906 0.447 0.886\\n          -          -          - 0.833 0.892 0.446 0.866\\nFirst Layer Last Layer True 0.825 0.883 0.443 0.861\\nAll Layers Last Layer True **0.823** 0.871 0.443 0.846\\nAll Layers All Layers True 0.828 **0.868** 0.443 **0.844**\\n\\n**Table 7 Ablations on the use of Cross Attention for a 1B BLT model trained on 100B bytes. We report bits-per-byte**\\n(bpb) on different datasets. We also report bpb on a random sample of the training data (denoted as Train Dist.) The\\nCross Attn. Enc. and Dec. columns denote which transformer layers the cross-attention block is applied after (or\\nbefore for the decoder) in the local encoder and decoder respectively.\\n\\nBPB\\n\\nNgram Sizes Per Ngram Vocab Total Vocab Wikipedia CC Github Train Dist\\n\\n     -      -      - 0.892 0.867 0.506 0.850\\n6,7,8 100k 300k 0.873 0.860 0.499 0.842\\n6,7,8 200k 600k 0.862 0.856 0.492 0.838\\n3,4,5 100k 300k 0.859 0.855 0.491 0.837\\n6,7,8 400k 1M 0.855 0.853 0.491 0.834\\n3,4,5 200k 600k 0.850 0.852 0.485 0.833\\n3,4,5,6,7,8 100k 600k 0.850 0.852 0.486 0.833\\n3,4,5 400k 1M 0.844 0.851 0.483 0.832\\n3,4,5,6,7,8 200k 1M 0.840 0.849 0.481 0.830\\n3,4,5,6,7,8 400k 2M **0.831** **0.846** **0.478** **0.826**\\n\\n**Table 8 Ablations on the use of n-gram hash embedding tables for a 1B BLT model trained on 100B bytes. We find**\\nthat hash n-gram embeddings are very effective with very large improvements in BPB. The most significant parameter\\nis the per-ngram vocab size and that smaller ngram sizes are more impactful than larger ones.\\n\\n_n-gram Hash Embeddings_ We ablate settings of 0, 100K, 200K and 400K n-gram hash embedding vocabularies\\nand present results in Table 8. We find that hash embeddings help on all domains, but particularly on\\nWikipedia and Github (0.04 bpb difference compared to 0.01 bpb difference after 15k steps at 8B). At 8B\\nscale going from 500K to 300K hashes changed performance by 0.001 bpb on 15k steps. This indicates that\\nhashes are vital to bringing the performance of BLT to match those of tokenizer based models, however, after\\n300K hashes, there are diminishing returns. Additionally, it appears that the gains are largely complementary\\nwith cross-attention as they provide improvements on different datasets.\\n\\n_Local Model Hyperparamaters_ In Table 9, we ablate various settings for the number of layers in the local\\nencoder and decoder. When paired with hash n-gram embeddings, BLT works well with an encoder that is\\nextremely light-weight i.e. just one layer, and with a heavier decoder.\\n\\n### 8 Related Work\\n\\n**Character-Level RNNs: Character Language Modeling has been a popular task ever since the early days of**\\nneural models (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013) owing to their flexibility of modeling\\nout of vocabulary words organically without resorting to back-off methods. Kim et al. (2016) also train a\\nmodel that processes characters only on the input side using convolutional and highway networks that feed\\ninto LSTM-based RNNs and are able to match performance with the RNN based state-of-the-art language\\nmodels of the time on English and outperform them on morphologically rich languages, another sought-after\\nadvantage of character-level LLMs. Kenter et al. (2018) do machine comprehension using byte-level LSTM\\n\\n\\n-----\\n\\nNgram Embeddings Encoder Layers Decoder Layers Train Dist BPB\\n\\nFalse 1 9 0.850\\nFalse 5 5 0.843\\n\\nTrue 5 5 0.844\\nTrue 3 7 0.824\\nTrue 1 9 0.822\\n\\n**Table 9 When paired with hash n-gram embeddings, a light-weight local encoder is sufficient. More layers can then be**\\nallocated to the decoder for the same cost.\\n\\nmodels that outperformed word-level models again on morphologically-rich Turkish and Russian languages.\\nAlong similar lines, Zhang et al. (2015) used character-based convolutional models for classification tasks,\\nwhich outperformed word-level models for certain tasks. Chung et al. (2019) use hierarchical LSTM models\\nusing boundary-detectors at each level to discover the latent hierarchy in text, to further improve performance\\non character level language modeling. ByteNet by Kalchbrenner et al. (2016) uses CNN based layers on\\ncharacters as opposed to attention for machine translation.\\n\\n**Character-Level Transformers: The development of transformer models using attention (Vaswani et al., 2017)**\\ntogether with subword tokenization (Sennrich et al., 2016), significantly improved the performance of neural\\nmodels on language modeling and benchmark tasks. However, word and sub-word units implicitly define an\\ninductive bias for the level of abstraction models should operate on. To combine the successes of transformer\\nmodels with the initial promising results on character language modeling, Al-Rfou et al. (2019) use very deep\\ntransformers, and with the help of auxiliary losses, train transformer-based models that outperformed previous\\nLSTM based character llms. However, they still saw a significant gap from word level LLMs. GPT-2 (Radford\\net al., 2019) also observed that on large scale datasets like the 1 billion word benchmark, byte-level LMs were\\nnot competitive with word-level LMs.\\n\\nWhile Choe et al. (2019) demonstrated that byte-level llms based on transformers can outperform subword\\nlevel LLMs with comparable parameters, the models take up much more compute and take much longer to\\ntrain. Similarly, El Boukkouri et al. (2020) train a BERT model (CharFormer) that builds word representations\\nby applying convolutions on character embeddings, and demonstrate improvements on the medical domain,\\nbut they also expend much more compute in doing so. Clark et al. (2022) develop CANINE, a 150M parameter\\nencoder-only model that operates directly on character sequences. CANINE uses a deep transformer stack at\\nits core similar in spirit to our global model, and a combination of a local transformer and strided convolutions\\nto downsample the input characters, and outperforms the equivalent token-level encoder-only model (mBERT)\\non downstream multilingual tasks. ByT5 (Xue et al., 2022) explored approaches for byte-level encoder decoder\\nmodels, that do not use any kind of patching operations. While their model exhibited improved robustness to\\nnoise, and was competitive with tokenizer-based models with 4x less data, the lack of patching meant that\\nthe models needed to compute expensive attention operations over every byte, which was extremely compute\\nheavy. Directly modeling bytes instead of subword units increases the sequence length of the input making it\\nchallenging to efficiently scale byte level models. Recently, using the Mamba Architecture (Gu and Dao, 2023),\\nwhich can maintain a fixed-size memory state over a very large context length, Wang et al. (2024) train a\\nbyte-level Mamba architecture also without using patching, and are able to outperform byte-level transformer\\nmodels in a flop controlled setting at the 350M parameter scale in terms of bits-per-byte on several datasets.\\n\\n**Patching-based approaches: The effective use of patching can bring down the otherwise inflated number of**\\nflops expended by byte-level LLMs while potentially retaining performance, and many works demonstrated\\ninitial successes at a small scale of model size and number of training bytes. Nawrot et al. (2022) experiment\\nwith static patching based downsampling and upsampling and develop the hourglass transformer which\\noutperforms other byte-level baselines at the 150M scale. Nawrot et al. (2023) further improve this with the\\nhelp of dynamic patching schemes, including a boundary-predictor that is learned in an end-to-end fashion, a\\nboundary-predictor supervised using certain tokenizers, as well as an entropy-based patching model similar to\\nBLT, and show that this approach can outperform the vanilla transformers of the time on language modeling\\ntasks at a 40M parameter scale on 400M tokens. Lester et al. (2024) investigate training on sequences\\n\\n\\n-----\\n\\ncompressed using arithmetic coding to achieve compression rates beyond what BPE can achieve, and by using\\na equal-info windows technique, are able to outperform byte-level baselines on language modeling tasks, but\\nunderperform subword baselines.\\n\\nOur work draws inspiration and is most closely related to MegaByte (Yu et al., 2023), which is a decoder only\\ncausal LLM that uses a fixed static patching and concatenation of representations to convert bytes to patches,\\nand uses a local model on the decoder side to convert from patches back into bytes. They demonstrate that\\nMegaByte can match tokenizer-based models at a 1B parameter scale on a dataset of 400B bytes. We ablate\\nMegaByte in all our experiments and find that static patching lags behind the current state-of-the-art compute\\noptimally trained tokenizer based models in a flop controlled setting and we demonstrate how BLT bridges\\nthis gap. Slagle (2024) make the same observation about MegaByte and suggest extending the static patching\\nmethod to patching on whitespaces and other space-like bytes, and also add a local encoder model. They find\\nimprovements over tokenized-based transformer models in a compute controlled setting on some domains such\\nas Github and arXiv at the 1B parameter scale. We also report experiments with this model, and show that\\nfurther architectural improvements are needed to scale up byte-level models even further and truly match\\ncurrent state-of-the-art token-based models such as Llama 3.\\n\\n### 9 Limitations and Future Work\\n\\nIn this work, for the purposes of architectural choices, we train models for the optimal number of steps as\\ndetermined for Llama 3 (Dubey et al., 2024). However, these scaling laws were calculated for BPE-level\\ntransformers and may lead to suboptimal (data, parameter sizes) ratios in the case of BLT. We leave for\\nfuture work the calculation of scaling laws for BLT potentially leading to even more favorable scaling trends\\nfor our architecture. Additionally, many of these experiments were conducted at scales upto 1B parameters,\\nand it is possible for the optimal architectural choices to change as we scale to 8B parameters and beyond,\\nwhich may unlock improved performance for larger scales.\\n\\nExisting transformer libraries and codebases are designed to be highly efficient for tokenizer-based transformer\\narchitectures. While we present theoretical flop matched experiments and also use certain efficient implementations (such as FlexAttention) to handle layers that deviate from the vanilla transformer architecture,\\nour implementations may yet not be at parity with tokenizer-based models in terms of wall-clock time and\\nmay benefit from further optimizations.\\n\\nWhile BLT uses a separately trained entropy model for patching, learning the patching model in an end-to-end\\nfashion can be an interesting direction for future work. In Section 6.2, we present initial experiments showing\\nindications of success for “byte-ifying” tokenizer-based models such as Llama 3 that are trained on more\\nthan 10T tokens, by initializing and freezing the global transformer with their weights. Further work in this\\ndirection may uncover methods that not only retain the benefits of bytefying, but also push performance\\nbeyond that of these tokenizer-based models without training them from scratch.\\n\\n### 10 Conclusion\\n\\nThis paper presents the Byte Latent Transformer (BLT), a new architecture that redefines the conventional\\ndependency on fixed-vocabulary tokenization in large language models. By introducing a dynamic, learnable\\nmethod for grouping bytes into patches, BLT effectively allocates computational resources based on data\\ncomplexity, leading to significant improvements in both efficiency and robustness. Our extensive scaling study\\ndemonstrates that BLT models can match the performance of tokenization-based models like Llama 3 at\\nscales up to 8B and 4T bytes, and can trade minor losses in evaluation metrics for up to 50% reductions in\\ninference flops. Furthermore, BLT unlocks a new dimension for scaling, allowing simultaneous increases in\\nmodel and patch size within a fixed inference budget. This new paradigm becomes advantageous for compute\\nregimes commonly encountered in practical settings. While directly engaging with raw byte data, BLT also\\nimproves the model’s ability to handle the long-tail of data, offering significant improvements in robustness to\\nnoisy inputs and a deeper understanding of sub-word structures. Overall, these results position BLT as a\\npromising alternative to traditional tokenization-based approaches, providing a scalable and robust framework\\nfor more efficient and adaptable language models.\\n\\n\\n-----\\n\\n### Acknowledgements\\n\\nWe would like to thank Kalyan Saladi for help with everything relating to pre-training infrastructure; Gabriel\\nSynnaeve, Ammar Rizvi, Jacob Kahn, Michel Meyer for helping organize resources for scaling up BLT; Badr\\nYoubi Idirissi, Mathurin Videau, and Jade Copet for invaluable discussions and feedback about BLT, for\\naccess to the Lingua framework for open-sourcing code for BLT, and for help preparing the BLT-1T dataset\\nused in this paper; Omer Levy, who was actively involved in the early stages of the project and provided\\nvaluable feedback and ideas; Driss Guessous for help with FlexAttention; and Sida Wang, Melanie Sclar,\\nAmanda Bertsch, and Hunter Lang for feedback and discussions.\\n\\n### Contributors\\n\\nIn this section, we list individual contributions.\\n\\n**Core Contributors:** Artidoro Pagnoni, Srinivasan Iyer, Ramakanth Pasunuru, Pedro Rodriguez, John Nguyen,\\nGargi Ghosh (Project Lead)\\n\\n**Core Advising Group:** Mike Lewis, Ari Holtzman, Luke Zettlemoyer\\n\\n**Advisors and Contributors:** Jason Weston, Benjamin Muller, Margaret Li, Chunting Zhou, Lili Yu\\n\\n\\n-----\\n\\n### References\\n\\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. Character-level language modeling with\\ndeeper self-attention. In Association for the Advancement of Artificial Intelligence, volume 33, pages 3159–3166,\\n2019.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie\\nCai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large language models, 2021.\\n\\nBing Bai, Jason Weston, David Grangier, Ronan Collobert, Kunihiko Sadamasa, Yanjun Qi, Olivier Chapelle, and\\nKilian Weinberger. Learning to rank with (a lot of) word features. Information retrieval, 13:291–314, 2010.\\n\\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural\\nlanguage. In Association for the Advancement of Artificial Intelligence, pages 7432–7439, 2020.\\n\\n[Adam Casson. Transformer flops, 2023. https://www.adamcasson.com/posts/transformer-flops.](https://www.adamcasson.com/posts/transformer-flops)\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\\nYuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy\\nKhlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power,\\nLukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex\\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher\\nHesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight,\\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya\\nSutskever, and Wojciech Zaremba. Evaluating large language models trained on code, 2021.\\n\\nDokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, and Noah Constant. Bridging the gap for tokenizer-free\\nlanguage models. arXiv, abs/1908.10322, 2019.\\n\\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In Proceedings\\n\\nof the International Conference on Learning Representations, 2019.\\n\\nJonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an efficient tokenization-free\\nencoder for language representation. Transactions of the Association for Computational Linguistics, 10:73–91, 2022.\\n\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.\\nThink you have solved question answering? Try ARC, the AI2 reasoning challenge. arXiv, 2018.\\n\\nGautier Dagan, Gabriel Synnaeve, and Baptiste Roziere. Getting the most out of your tokenizer for pre-training and\\ndomain adaptation. In Forty-first International Conference on Machine Learning, 2024.\\n\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. FlashAttention: Fast and memory-efficient exact\\nattention with io-awareness. Proceedings of Advances in Neural Information Processing Systems, 35, 2022.\\n\\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil\\nMathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv, 2024.\\n\\nLukas Edman, Helmut Schmid, and Alexander Fraser. CUTE: Measuring llms’ understanding of their tokens. arXiv,\\n2024.\\n\\nHicham El Boukkouri, Olivier Ferret, Thomas Lavergne, Hiroshi Noji, Pierre Zweigenbaum, and Jun’ichi Tsujii.\\nCharacterBERT: Reconciling elmo and bert for word-level open-vocabulary representations from characters. In\\nProceedings of International Conference on Computational Linguistics, 2020.\\n\\nPhilip Gage. A new algorithm for data compression. The C Users Journal, 12(2):23–38, 1994.\\n\\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan,\\nMarc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. The flores-101 evaluation benchmark for low-resource\\nand multilingual machine translation. 2022.\\n\\nAlex Graves. Generating sequences with recurrent neural networks. arXiv, 2013.\\n\\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv, 2023.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.\\nMeasuring massive multitask language understanding. In Proceedings of the International Conference on Learning\\nRepresentations, 2020.\\n\\n\\n-----\\n\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego\\nde Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language\\nmodels. In Proceedings of Advances in Neural Information Processing Systems, 2022.\\n\\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General\\nperception with iterative attention. In Proceedings of the International Conference of Machine Learning. PMLR,\\n2021.\\n\\nNal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aäron van den Oord, Alexander Graves, and Koray Kavukcuoglu.\\nNeural machine translation in linear time. arXiv, 2016.\\n\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec\\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv, 2020.\\n\\nTom Kenter, Llion Jones, and Daniel Hewlett. Byte-level machine reading across morphologically varied languages. In\\n\\nAssociation for the Advancement of Artificial Intelligence, 2018.\\n\\nYoon Kim, Yacine Jernite, David Sontag, and Alexander Rush. Character-aware neural language models. In Association\\n\\nfor the Advancement of Artificial Intelligence, 2016.\\n\\nBrian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, and Noah Constant.\\nTraining llms over neurally compressed text. arXiv, 2024.\\n\\nJeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick\\nKeh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models.\\narXiv, 2024.\\n\\nDavis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian\\nKhabsa. Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models. In Proceedings of\\nEmpirical Methods in Natural Language Processing, 2023.\\n\\nTomasz Limisiewicz, Terra Blevins, Hila Gonen, Orevaoghene Ahia, and Luke Zettlemoyer. Myte: Morphology-driven\\nbyte encoding for better and fairer multilingual language modeling. arXiv, 2024.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv, 2017.\\n\\nTomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-Son Le, Stefan Kombrink, and Jan Cernocky. Subword language\\nmodeling with neural networks. preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char. pdf), 8(67), 2012.\\n\\nPiotr Nawrot, Szymon Tworkowski, Michał Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk\\nMichalewski. Hierarchical transformers are more efficient language models. In Conference of the North American\\nChapter of the Association for Computational Linguistics. Association for Computational Linguistics, 2022.\\n\\nPiotr Nawrot, Jan Chorowski, Adrian Lancucki, and Edoardo Maria Ponti. Efficient transformers with dynamic\\ntoken pooling. In Proceedings of the Association for Computational Linguistics. Association for Computational\\nLinguistics, 2023.\\n\\nAleksandar Petrov, Emanuele La Malfa, Philip Torr, and Adel Bibi. Language model tokenizers introduce unfairness\\nbetween languages. Proceedings of Advances in Neural Information Processing Systems, 2024.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are\\nunsupervised multitask learners. 2019.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In\\n\\nProceedings of the Association for Computational Linguistics. Association for Computational Linguistics, 2016.\\n\\nNoam Shazeer. GLU variants improve transformer. arXiv, 2020.\\n\\nKevin Slagle. Spacebyte: Towards deleting tokenization from large language modeling. arXiv, 2024.\\n\\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. RoFormer: Enhanced transformer\\nwith rotary position embedding. arxiv e-prints, art. arXiv, 2021.\\n\\nIlya Sutskever, James Martens, and Geoffrey E Hinton. Generating text with recurrent neural networks. In Proceedings\\n\\nof the International Conference of Machine Learning, pages 1017–1024, 2011.\\n\\nAshima Suvarna, Harshita Khandelwal, and Nanyun Peng. Phonologybench: Evaluating phonological skills of large\\nlanguage models. arXiv, 2024.\\n\\n\\n-----\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\\narXiv, 2023.\\n\\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and\\nIllia Polosukhin. Attention is all you need. 2017.\\n\\nJunxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, and Alexander M Rush. Mambabyte: Token-free selective\\nstate space model. arXiv, 2024.\\n\\nWenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta,\\nKarthik Abinav Sankararaman, Barlas Oguz, et al. Effective long-context scaling of foundation models. In Conference\\nof the North American Chapter of the Association for Computational Linguistics, 2024.\\n\\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin\\nRaffel. Byt5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for\\nComputational Linguistics, 10:291–306, 2022.\\n\\nLili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting\\nmillion-byte sequences with multiscale transformers. Proceedings of Advances in Neural Information Processing\\nSystems, 2023.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your\\nsentence? arXiv, 2019.\\n\\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. Proceedings of Advances in Neural Information\\n\\nProcessing Systems, 32, 2019.\\n\\nXiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In\\nC. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Proceedings of Advances in Neural\\n[Information Processing Systems, volume 28. Curran Associates, Inc., 2015. https://proceedings.neurips.cc/paper_](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)\\n[files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)\\n\\n\\n-----\\n\\n# Appendix\\n\\n### A Model Hyper Parameters\\n\\nTable 10 shows different hyper parameter settings for BLT models.\\n\\nEncoder Global Latent Transf. Decoder Cross-Attn.\\nModel _lE_ #heads _hE_ #Params _lG_ #heads _hG_ #Params _lD_ #heads _hD_ #Params #heads k\\n\\n**400M** 1 12 768 7M 24 10 1280 470M 7 12 768 50M 10 2\\n**1B** 1 16 1024 12M 25 16 2048 1B 9 16 1024 113M 16 2\\n**2B** 1 16 1024 12M 26 20 2560 2B 9 16 1024 113M 16 3\\n**4B** 1 16 1024 12M 36 24 3072 4.1B 9 16 1024 113M 16 3\\n**8B** 1 20 1280 20M 32 32 4096 6.4B 6 20 1280 120M 20 4\\n\\n**Table 10 Architectural hyper-parameters for different BLT model sizes that we train for flop-controlled experiments**\\ndescribed in this paper.\\n\\n### B FLOPs Equations\\n\\nHere, we provide the equations used for flop computation for the forward-pass of transformer and BLT\\nmodels based on Hoffmann et al. (2022); Kaplan et al. (2020); Casson (2023). We assume that the backward\\npass uses twice as much flops as the forward pass.\\n\\nOperation flops per token/byte\\n\\nAttention (l, hk, nheads, m) 4 × l × hk × nheads × _[m]2[+1]_\\n\\nQKVO (l, h, r) (r × 2 + 2) × 2 × l × h[2]\\n\\nFeed-forward (l, h, dff ) 2 × l × 2 × h × dff _h_\\nDe-Embedding (h, V ) 2 × h × |V |\\nCross-Attention (l, hk, nheads, p, r) Attention(l, hk, nheads, p) + QKVO(l, hk × nheads, r)\\n\\n**Table 11 flops for operations used in transformer and BLT models. l corresponds to layers, h is the hidden dimension**\\n(hk with nheads heads), m is the context length, dff = 4 is the feed-forward dimension multiplier, p is the patch size,\\nand r is the ratio of queries to keys.\\n\\nFor a transformer model with l layers, hidden dimension h, context length m, nheads attention heads of\\ndimension hk, and a feed-forward multipler of dff, we compute flops as:\\n\\nTransformer-FLOPs(l, h, m, nheads, hk, dff _, V ) = Feed-forward(l, h, dff_ ) (19)\\n\\n+ QKVO(l, h, r = 1) (20)\\n\\n+ Attention(l, hk, nheads, m) (21)\\n\\n+ De-Embedding(h, V ) (22)\\n\\nFor BLT models, we use the above-mentioned primitives together with the flops equation from Section 4.5\\nto compute total flops.\\n\\n### C Rolling Polynomial Hashing\\n\\nGiven a byte n-gram gi,n = {bi−n+1, . . ., bi}, the rolling polynomial hash of gi,n is defined as:\\n\\n\\nHash(gi,n) =\\n\\nWhere a is chosen to be a 10-digit prime number.\\n\\n\\n_n_\\n� _bi−j+1a[j][−][1]_ (23)\\n\\n_j=1_\\n\\n\\n-----\\n\\n### D Frequency-based n-gram Embedddings\\n\\nPrior to using hash n-gram embeddings in the final BLT architecture, we also experimented with frequencybased n-gram embeddings. For each n ∈{1, 2, 3, 4, 5, 6, 7, 8} there is an embedding matrix En[ngram] that\\ncontains the most frequent byte-grams for the given n. Since it is intractable to store embeddings as n grows,\\nwe only store embeddings for the most frequent 100, 000 byte-grams for each byte-gram. If a particular\\nposition i includes an n-gram present in the corresponding the embedding matrix, then this embedding is\\npassed to the next step, encoder multi-headed cross-attention. If a byte-gram is infrequent and therefore not\\nin the matrix, then its embedding is obtained from encoder hash embeddings instead.\\n\\nSince frequency-based n-grams are limited by the vocabulary of the n-gram tables with infrequent n-grams\\nnot being represented at all, we subsequently moved to hash-based n-gram embeddings. See Table 12 for a\\ncomparison of hash and frequency based n-gram embeddings.\\n\\nbpb\\n\\nHash Ngram Sizes Per Hash Ngram Vocab Ngram Sizes Per Ngram Vocab Total Vocab Wikipedia CC Github Train Dist\\n\\n - - - - - 0.892 0.867 0.506 0.850\\n6,7,8 50k 6,7,8 50k 300k 0.878 0.860 0.497 0.843\\n6,7,8 100k - - 300k 0.873 0.860 0.499 0.842\\n6,7,8 100k 6,7,8 100k 600k 0.868 0.857 0.494 0.839\\n6,7,8 200k - - 600k 0.862 0.856 0.492 0.838\\n3,4,5 50k 3,4,5 50k 300k 0.862 0.856 0.491 0.837\\n3,4,5 100k - - 300k 0.859 0.855 0.491 0.837\\n6,7,8 200k 6,7,8 200k 1M 0.861 0.855 0.491 0.837\\n6,7,8 400k - - 1M 0.855 0.853 0.491 0.834\\n3,4,5,6,7,8 50k 3,4,5,6,7,8 50k 600k 0.855 0.853 0.488 0.834\\n3,4,5 100k 3,4,5 100k 600k 0.851 0.853 0.486 0.834\\n3,4,5 200k - - 600k 0.850 0.852 0.485 0.833\\n3,4,5,6,7,8 100k - - 600k 0.850 0.852 0.486 0.833\\n3,4,5 400k - - 1M 0.844 0.851 0.483 0.832\\n3,4,5 200k 3,4,5 200k 1M 0.843 0.850 0.482 0.830\\n3,4,5,6,7,8 100k 3,4,5,6,7,8 100k 1M 0.844 0.850 0.482 0.830\\n3,4,5,6,7,8 200k - - 1M 0.840 0.849 0.481 0.830\\n3,4,5,6,7,8 200k 3,4,5,6,7,8 200k 2M **0.833** **0.846** **0.478** **0.826**\\n3,4,5,6,7,8 400k - - 2M **0.831** **0.846** **0.478** **0.826**\\n\\n**Table 12 Ablations on the use of frequency-based as well as hash-based n-gram embedding tables for a 1B BLT model**\\ntrained on 100B bytes.\\n\\n### E Entropy Patching Example from MMLU\\n\\nWe illustrate how a few-shot example from a downstream task i.e. MMLU (Hendrycks et al., 2020), is patched\\nusing an entropy-model trained for use with BLT models in Figure 9. Directly using the entropy model with\\nthe full-context window causes repetitive patterns to be heavily patched. For example, “10 times, with an rms\\ndeviation of about” in the MMLU query is patched frequently the first time it is encountered, but is part of\\nvery large patches the next three times, which, although inference efficient, maybe undesirable for reasoning.\\nOne method that we use to avoid such a “entropy” drift is by resetting the entropy context with new lines and\\nusing a approximate monotonicity constraint (see Section 4.4).\\n\\n\\n-----\\n\\n**Figure 9 An example of default entropy-based patching with global threshold during inference on mmlu. Green denotes**\\nthe prompt, Blue denotes the few-shot examples, and red denotes the question to be answered. Note that the size\\nof the patches for the repeated phrases in the answer choices is much larger, which means that the global model is\\ninvoked significantly fewer times than its tokenizer-based counterpart, with this inference patching scheme.\\n\\n\\n-----\\n\\n\\n```\\n\\n'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.set_prompt(text=prompt)\n",
    "client.get_prompt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
